<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.35" />


<title>The Game of Kaggle - A Hugo website</title>
<meta property="og:title" content="The Game of Kaggle - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">19 min read</span>
    

    <h1 class="article-title">The Game of Kaggle</h1>

    
    <span class="article-date">2016/05/21</span>
    

    <div class="article-content">
      

<p>{% include _toc.html %}</p>

<h2 id="the-player-of-games">The Player of Games</h2>

<p>What&rsquo;s the best way to teach oneself machine learning? Is it to do an <a href="https://www.coursera.org/specializations/jhu-data-science">online course</a>, write a <a href="http://www.machinegurning.com/posts/">blog</a> or compete in <a href="https://www.kaggle.com/">online programming competitions</a>?</p>

<p>This blog post describes my first interaction with / or game of <a href="https://www.kaggle.com/">Kaggle</a>.</p>

<h2 id="kaggle-history">Kaggle history</h2>

<p>In 2010, Kaggle was founded as a platform for predictive modelling and analytics competitions on which companies and researchers post their data and statisticians and data miners from all over the world compete to produce the best models. This crowdsourcing approach relies on the fact that there are countless strategies that can be applied to any predictive modelling task and it is impossible to know at the outset which technique or analyst will be most effective.</p>

<h2 id="gameification">Gameification</h2>

<p>I was interested in the gaming element and how user&rsquo;s of Kaggle describe themselves as players. This blog post describes my first foray into Kaggle with the well known <a href="https://www.kaggle.com/c/titanic/">Titanic survival problem</a>.</p>

<h2 id="a-titanic-problem">A Titanic problem</h2>

<p>The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.</p>

<p>One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking (<a href="http://www.discovery.com/tv-shows/mythbusters/videos/titanic-survival-results/">Rose found a door to float on</a>), some groups of people were more likely to survive than others, such as women, children, and the upper-class.</p>

<p>In this challenge, Kaggle ask us to complete the analysis of what sorts of people were likely to survive. In particular, they ask us to apply the tools of machine learning to predict which passengers survived the tragedy.</p>

<p>Challenge accepted!</p>

<h2 id="getting-the-data">Getting the data</h2>

<p>The data can be downloaded from <a href="https://www.kaggle.com/c/titanic/data">here</a>. I saved the files into a newly created directory called titanic in my R folder. We are provided with <code>train</code> and <code>test</code> data; we train our predictive model then test our ability to accurately predict whether a passenger is likely to survive or not based on their characteristics in thetest data (and assess how our model performs when it processes those characteristics and makes a prediction of the fate of the passenger).</p>

<p>{% highlight r %}
#LIBRARY
library(vcd)
library(RColorBrewer)
library(dplyr)</p>

<p>#INPUT
train &lt;- read.csv(&ldquo;data/2016-05-21-tit_train.csv&rdquo;, header = TRUE)
test &lt;- read.csv(&ldquo;data/2016-05-21-tit_test.csv&rdquo;, header = TRUE)
{% endhighlight %}</p>

<h2 id="passenger-characteristics">Passenger characteristics</h2>

<p>If we compare the training and testing datasets we notice that the training set has an additional variable. This is the response variable we are trying to predict in this supervised learning task. Our machine learning algorithms need to identify characteristics which are important in determining whether a passenger is likely to survive (<code>Survived</code> = 1) or not (<code>Survived</code> = 0).</p>

<p>{% highlight r %}
str(train)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="data-frame-891-obs-of-12-variables">&lsquo;data.frame&rsquo;:    891 obs. of  12 variables:</h2>

<h2 id="passengerid-int-1-2-3-4-5-6-7-8-9-10">$ PassengerId: int  1 2 3 4 5 6 7 8 9 10 &hellip;</h2>

<h2 id="survived-int-0-1-1-1-0-0-0-0-1-1">$ Survived   : int  0 1 1 1 0 0 0 0 1 1 &hellip;</h2>

<h2 id="pclass-int-3-1-3-1-3-3-1-3-3-2">$ Pclass     : int  3 1 3 1 3 3 1 3 3 2 &hellip;</h2>

<h2 id="name-factor-w-891-levels-abbing-mr-anthony-109-191-358-277-16-559-520-629-417-581">$ Name       : Factor w/ 891 levels &ldquo;Abbing, Mr. Anthony&rdquo;,..: 109 191 358 277 16 559 520 629 417 581 &hellip;</h2>

<h2 id="sex-factor-w-2-levels-female-male-2-1-1-1-2-2-2-2-1-1">$ Sex        : Factor w/ 2 levels &ldquo;female&rdquo;,&ldquo;male&rdquo;: 2 1 1 1 2 2 2 2 1 1 &hellip;</h2>

<h2 id="age-num-22-38-26-35-35-na-54-2-27-14">$ Age        : num  22 38 26 35 35 NA 54 2 27 14 &hellip;</h2>

<h2 id="sibsp-int-1-1-0-1-0-0-0-3-0-1">$ SibSp      : int  1 1 0 1 0 0 0 3 0 1 &hellip;</h2>

<h2 id="parch-int-0-0-0-0-0-0-0-1-2-0">$ Parch      : int  0 0 0 0 0 0 0 1 2 0 &hellip;</h2>

<h2 id="ticket-factor-w-681-levels-110152-110413-524-597-670-50-473-276-86-396-345-133">$ Ticket     : Factor w/ 681 levels &ldquo;110152&rdquo;,&ldquo;110413&rdquo;,..: 524 597 670 50 473 276 86 396 345 133 &hellip;</h2>

<h2 id="fare-num-7-25-71-28-7-92-53-1-8-05">$ Fare       : num  7.25 71.28 7.92 53.1 8.05 &hellip;</h2>

<h2 id="cabin-factor-w-148-levels-a10-a14-1-83-1-57-1-1-131-1-1-1">$ Cabin      : Factor w/ 148 levels &ldquo;&rdquo;,&ldquo;A10&rdquo;,&ldquo;A14&rdquo;,..: 1 83 1 57 1 1 131 1 1 1 &hellip;</h2>

<h2 id="embarked-factor-w-4-levels-c-q-s-4-2-4-4-4-3-4-4-4-2">$ Embarked   : Factor w/ 4 levels &ldquo;&rdquo;,&ldquo;C&rdquo;,&ldquo;Q&rdquo;,&ldquo;S&rdquo;: 4 2 4 4 4 3 4 4 4 2 &hellip;</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
str(test)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="data-frame-418-obs-of-11-variables">&lsquo;data.frame&rsquo;:    418 obs. of  11 variables:</h2>

<h2 id="passengerid-int-892-893-894-895-896-897-898-899-900-901">$ PassengerId: int  892 893 894 895 896 897 898 899 900 901 &hellip;</h2>

<h2 id="pclass-int-3-3-2-3-3-3-3-2-3-3">$ Pclass     : int  3 3 2 3 3 3 3 2 3 3 &hellip;</h2>

<h2 id="name-factor-w-418-levels-abbott-master-eugene-joseph-210-409-273-414-182-370-85-58-5-104">$ Name       : Factor w/ 418 levels &ldquo;Abbott, Master. Eugene Joseph&rdquo;,..: 210 409 273 414 182 370 85 58 5 104 &hellip;</h2>

<h2 id="sex-factor-w-2-levels-female-male-2-1-2-2-1-2-1-2-1-2">$ Sex        : Factor w/ 2 levels &ldquo;female&rdquo;,&ldquo;male&rdquo;: 2 1 2 2 1 2 1 2 1 2 &hellip;</h2>

<h2 id="age-num-34-5-47-62-27-22-14-30-26-18-21">$ Age        : num  34.5 47 62 27 22 14 30 26 18 21 &hellip;</h2>

<h2 id="sibsp-int-0-1-0-0-1-0-0-1-0-2">$ SibSp      : int  0 1 0 0 1 0 0 1 0 2 &hellip;</h2>

<h2 id="parch-int-0-0-0-0-1-0-0-1-0-0">$ Parch      : int  0 0 0 0 1 0 0 1 0 0 &hellip;</h2>

<h2 id="ticket-factor-w-363-levels-110469-110489-153-222-74-148-139-262-159-85-101-270">$ Ticket     : Factor w/ 363 levels &ldquo;110469&rdquo;,&ldquo;110489&rdquo;,..: 153 222 74 148 139 262 159 85 101 270 &hellip;</h2>

<h2 id="fare-num-7-83-7-9-69-8-66-12-29">$ Fare       : num  7.83 7 9.69 8.66 12.29 &hellip;</h2>

<h2 id="cabin-factor-w-77-levels-a11-a18-1-1-1-1-1-1-1-1-1-1">$ Cabin      : Factor w/ 77 levels &ldquo;&rdquo;,&ldquo;A11&rdquo;,&ldquo;A18&rdquo;,..: 1 1 1 1 1 1 1 1 1 1 &hellip;</h2>

<h2 id="embarked-factor-w-3-levels-c-q-s-2-3-2-3-3-3-2-3-1-3">$ Embarked   : Factor w/ 3 levels &ldquo;C&rdquo;,&ldquo;Q&rdquo;,&ldquo;S&rdquo;: 2 3 2 3 3 3 2 3 1 3 &hellip;</h2>

<p>{% endhighlight %}</p>

<p>This data has a lot more to it than the Titanic dataset which comes with the datasets package in R.</p>

<p>{% highlight r %}
Titanic
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="age-child-survived-no">, , Age = Child, Survived = No</h2>

<h2 id="sex">Sex</h2>

<h2 id="class-male-female">Class  Male Female</h2>

<h2 id="1st-0-0">1st     0      0</h2>

<h2 id="2nd-0-0">2nd     0      0</h2>

<h2 id="3rd-35-17">3rd    35     17</h2>

<h2 id="crew-0-0">Crew    0      0</h2>

<h2 id="age-adult-survived-no">, , Age = Adult, Survived = No</h2>

<h2 id="sex-1">Sex</h2>

<h2 id="class-male-female-1">Class  Male Female</h2>

<h2 id="1st-118-4">1st   118      4</h2>

<h2 id="2nd-154-13">2nd   154     13</h2>

<h2 id="3rd-387-89">3rd   387     89</h2>

<h2 id="crew-670-3">Crew  670      3</h2>

<h2 id="age-child-survived-yes">, , Age = Child, Survived = Yes</h2>

<h2 id="sex-2">Sex</h2>

<h2 id="class-male-female-2">Class  Male Female</h2>

<h2 id="1st-5-1">1st     5      1</h2>

<h2 id="2nd-11-13">2nd    11     13</h2>

<h2 id="3rd-13-14">3rd    13     14</h2>

<h2 id="crew-0-0-1">Crew    0      0</h2>

<h2 id="age-adult-survived-yes">, , Age = Adult, Survived = Yes</h2>

<h2 id="sex-3">Sex</h2>

<h2 id="class-male-female-3">Class  Male Female</h2>

<h2 id="1st-57-140">1st    57    140</h2>

<h2 id="2nd-14-80">2nd    14     80</h2>

<h2 id="3rd-75-76">3rd    75     76</h2>

<h2 id="crew-192-20">Crew  192     20</h2>

<p>{% endhighlight %}</p>

<p>Tables are not that effective for spotting patterns in the data. Plotting can be helpful even for complicated Multivariate Categorical data. Let&rsquo;s have a look at our training data.</p>

<blockquote>
<p>&ldquo;Women and children first&rdquo; (or to a lesser extent, the <a href="http://www.phrases.org.uk/meanings/women-and-children-first.html">Birkenhead Drill</a>).</p>
</blockquote>

<p>{% highlight r %}
colours &lt;- brewer.pal(6, &ldquo;Paired&rdquo;) #  some pretty colours, qualitative differences</p>

<p>doubledecker(Survived ~ Sex, data = train,
             gp = gpar(fill = colours))
{% endhighlight %}</p>

<p><img src="/figures/2016-05-21_dd1-1.svg" alt="plot of chunk 2016-05-21_dd1" /></p>

<p>The relative width of the bars in the mosaic plot tells us that there were more men on board the Titanic (this difference in width is a limitation of the plot as humans struggle to discriminate between different sized areas). Also it is apparent that if you were a randomly selected woman you were more likely to survive (<code>Survived</code> = 1) than a randomly selected male (who was more likely to die, <code>Survived</code> = 0). This could be the basis of a simple predictive tool to use on the test data if we were feeling lazy. We pursue this this here to outline how one would participate in a Kaggle problem.</p>

<p>This simplistic approach can be modelled using a logistic regression AKA generalised linear model with binomial error distribution and logit link function. We can use the <code>coef()</code> function in order to access just the coefficients. The estimates and their standard errors are in logits. We can use the <code>predict()</code> function to predict the probability that the passengers in the test data survive or die based purely on their sex. The higher probabilities should pair with female and the lower with male given our simplistic model.</p>

<p>{% highlight r %}
m1 &lt;- glm(Survived~Sex, family = &ldquo;quasibinomial&rdquo;, data = train)
m1_probs &lt;- predict(m1, test, type = &ldquo;response&rdquo;)
head(m1_probs)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-2-3-4-5-6">1         2         3         4         5         6</h2>

<h2 id="0-1889081-0-7420382-0-1889081-0-1889081-0-7420382-0-1889081">0.1889081 0.7420382 0.1889081 0.1889081 0.7420382 0.1889081</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
head(test$Sex)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-male-female-male-male-female-male">[1] male   female male   male   female male</h2>

<h2 id="levels-female-male">Levels: female male</h2>

<p>{% endhighlight %}</p>

<p>However, passengers either <code>Survived</code> (= 1) or they died (<code>Survived</code> = 0), none of this probability malarky. Thus we convert our predictions into a 1 if greater than 0.5, or a zero if less than or equal to 0.5. Then we would tabulate against the actual <code>Survive</code> status of the test passengers, if we had it&hellip;</p>

<p>{% highlight r %}
m1_pred &lt;- rep(x = 0, nrow(test))  # assume all test passengers dead,
m1_pred[m1_probs &gt; 0.5] &lt;- 1  #  unless they are female, or their probability of Survived
                               #  is greater than 0.5
table(test$Sex)  #  this table should match the Sex table for the test data
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="female-male">female   male</h2>

<h2 id="152-266">152    266</h2>

<p>{% endhighlight %}</p>

<p>Kaggle is great, no peeking at the test data! How do we go about submitting our predictions?</p>

<p>{% highlight r %}
  #  First we create a new variable in test
  #  predicted Survived variable in test
  #  we then create a dataframe with the passenger id paired with our survival prediciton
test$Survived &lt;- m1_pred
submit &lt;- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = &ldquo;submission.csv&rdquo;, row.names = FALSE)
{% endhighlight %}</p>

<h2 id="submission">Submission</h2>

<p>To submit your predictions go to the <a href="https://www.kaggle.com/c/titanic/submissions/attach">Titanic submission page</a> and upload your file. This got me a respectable accuracy of ~77% and ranked me in the mid 3000 (of 4000 or so) which you can easily <a href="https://twitter.com/mammykins_/status/733003027355971585">Tweet</a> to impress your friends. Interestingly some players managed to get a prediction accuracy of 0%&hellip;</p>

<p>That was just a quick demo to show how you might go about this problem. Let&rsquo;s continue to look at the data and then put in a bit more effort in to generating a suitable model that accounts for the relevant characteristics of the passengers and how that affects their probability of survival. I&rsquo;m interested in investigating how far we can get in improving model accuracy with using simple out of the box strategies and how far we can get up the leaderboard! I don&rsquo;t expect to win but as a data scientist I am keen to learn strategies and processes that result in the most significant gains in predictive accuracy through our variety of modelling problems - Kaggle provides just that!</p>

<h3 id="learning-through-gaming-on-kaggle">Learning through gaming on Kaggle</h3>

<p>This simplistic model aside, an important aspect of machine learning that Kaggle elucidates is how easy is it to make a decent predictive model using out of the box algorithms and techniques, even if we are new to the art. There are plenty of blog posts which expand on this Titanic data set and come up with clever ways of improving model performance. <a href="http://www.r-bloggers.com/titanic-kaggle-competition-pt-2%ef%bb%bf/">Feature engineering</a> is particularly neat.</p>

<h3 id="class-effects">Class effects?</h3>

<p>{% highlight r %}
doubledecker(Survived ~ Pclass, data = train,
             gp = gpar(fill = colours))
{% endhighlight %}</p>

<p><img src="/figures/2016-05-21_dd2-1.svg" alt="plot of chunk 2016-05-21_dd2" /></p>

<p>Do these individual graphics miss anything? Icluding a main effect of <code>Pclass</code> might be useful.</p>

<p>What about class and sex effects and the interaction between the two?</p>

<p>{% highlight r %}
doubledecker(Survived ~ Sex + Pclass, data = train,
             gp = gpar(fill = colours))
{% endhighlight %}</p>

<p><img src="/figures/2016-05-21_dd3-1.svg" alt="plot of chunk 2016-05-21_dd3" /></p>

<p>There appears to be an ineteraction with the class of the sexes associated with different chance of survival.</p>

<h3 id="improving-the-model">Improving the model</h3>

<p>We iterate the model and improve. Exploring the data visually identifies variables that might improve the model if incorporated. We include passenger sex, passenger class and the interaction between the two in the model.</p>

<p>{% highlight r %}</p>

<h1 id="build-model-assign-probabilities-of-survive-given-the-passenger-characteristics">build model, assign probabilities of Survive given the passenger characteristics</h1>

<h1 id="then-create-a-vector-to-hold-our-passenger-survive-predictions">then create a vector to hold our passenger survive predictions</h1>

<p>m2 &lt;- glm(Survived~Sex*Pclass, family = &ldquo;binomial&rdquo;, data = train)
summary(m2)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="call">Call:</h2>

<h2 id="glm-formula-survived-sex-pclass-family-binomial-data-train">glm(formula = Survived ~ Sex * Pclass, family = &ldquo;binomial&rdquo;, data = train)</h2>

<h2 id="deviance-residuals">Deviance Residuals:</h2>

<h2 id="min-1q-median-3q-max">Min       1Q   Median       3Q      Max</h2>

<h2 id="2-8488-0-6969-0-5199-0-4946-2-0339">-2.8488  -0.6969  -0.5199   0.4946   2.0339</h2>

<h2 id="coefficients">Coefficients:</h2>

<h2 id="estimate-std-error-z-value-pr-z">Estimate Std. Error z value Pr(&gt;|z|)</h2>

<h2 id="intercept-6-0416-0-8273-7-303-2-81e-13">(Intercept)      6.0416     0.8273   7.303 2.81e-13 ***</h2>

<h2 id="sexmale-6-0493-0-8756-6-909-4-88e-12">Sexmale         -6.0493     0.8756  -6.909 4.88e-12 ***</h2>

<h2 id="pclass-2-0011-0-2950-6-784-1-17e-11">Pclass          -2.0011     0.2950  -6.784 1.17e-11 ***</h2>

<h2 id="sexmale-pclass-1-3593-0-3202-4-245-2-19e-05">Sexmale:Pclass   1.3593     0.3202   4.245 2.19e-05 ***</h2>

<h2 id="toc_82">&mdash;</h2>

<h2 id="signif-codes-0-0-001-0-01-0-05-0-1-1">Signif. codes:  0 &lsquo;*<strong>&rsquo; 0.001 &lsquo;</strong>&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1</h2>

<h2 id="dispersion-parameter-for-binomial-family-taken-to-be-1">(Dispersion parameter for binomial family taken to be 1)</h2>

<h2 id="null-deviance-1186-66-on-890-degrees-of-freedom">Null deviance: 1186.66  on 890  degrees of freedom</h2>

<h2 id="residual-deviance-803-12-on-887-degrees-of-freedom">Residual deviance:  803.12  on 887  degrees of freedom</h2>

<h2 id="aic-811-12">AIC: 811.12</h2>

<h2 id="number-of-fisher-scoring-iterations-6">Number of Fisher Scoring iterations: 6</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
m2_probs &lt;- predict(m2, test, type = &ldquo;response&rdquo;)
m2_pred &lt;- rep(x = 0, nrow(test))
m2_pred[m2_probs &gt; 0.5] &lt;- 1</p>

<h1 id="write-to-file-for-submission">write to file for submission</h1>

<p>test$Survived &lt;- m2_pred
submit &lt;- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = &ldquo;submission2.csv&rdquo;, row.names = FALSE)
{% endhighlight %}</p>

<p>We submit our new improved model. This results in exactly the same predictions for each test passenger! We have mot improved our position in the scoreboard. If we look closely we see that we are still not getting enough information to distinguish any nuance, as there are two sexes and three classes, we only get six different possible probabilities from the logistic regression based on the coefficients from our model fit to the training data. Although if we inspect the coefficients we actually have fewer than that&hellip;</p>

<p>{% highlight r %}
table(round(m2_probs, digits = 0))  #  the same as sex
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="0-1">0   1</h2>

<h2 id="266-152">266 152</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
table(round(m2_probs, digits = 2))
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="0-13-0-22-0-34-0-51-0-88-0-98">0.13 0.22 0.34 0.51 0.88 0.98</h2>

<h2 id="146-63-57-72-30-50">146   63   57   72   30   50</h2>

<p>{% endhighlight %}</p>

<h3 id="model-interpretation">Model Interpretation</h3>

<p>The coefficients are from the linear predictor. They are on the transformed scale, so because we are using binoimial errors they are in logits. We convert from a logit to a proportion.</p>

<p>{% highlight r %}
round(1/(1 + 1/exp(m2$coefficients)), digits = 5)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="intercept-sexmale-pclass-sexmale-pclass">(Intercept)        Sexmale         Pclass Sexmale:Pclass</h2>

<h2 id="0-99763-0-00235-0-11909-0-79564">0.99763        0.00235        0.11909        0.79564</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
head(train$Sex)  #  female comes before male alphabetically
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-male-female-female-female-male-male">[1] male   female female female male   male</h2>

<h2 id="levels-female-male-1">Levels: female male</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
head(train$Pclass)  #  interestingly an integer, not a factor
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-3-1-3-1-3-3">[1] 3 1 3 1 3 3</h2>

<p>{% endhighlight %}</p>

<p>The <code>(intercept)</code> says that the mean survival rate of first class females (<code>female</code> before <code>male</code> alphabetically, with <code>Pclass</code> a variable of class integer) was 99.8%. First class females were unlikely to die.</p>

<p>What about the parameter for <code>Sexmales</code>? Remember that with categorical explanatory variables the parameter values are differences between means. So to get the male survival rate we add 0.00235 to the intercept before back transforming.</p>

<p>{% highlight r %}
1 / (1 + 1 / exp((m2$coefficients[1]) + (m2$coefficients[2])))
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="intercept">(Intercept)</h2>

<h2 id="0-498075">0.498075</h2>

<p>{% endhighlight %}</p>

<p>This suggests that the survival rate for men was almost half that of women.</p>

<p>What about class? For every unit change in <code>Pclass</code>, as you move between classes from higher (1), to middle (2) to lower (3), the log odds of survival decreases.</p>

<p>{% highlight r %}
m2$coefficients[&ldquo;Pclass&rdquo;]
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="pclass">Pclass</h2>

<h2 id="2-00112">-2.00112</h2>

<p>{% endhighlight %}</p>

<p>There is also a significant interaction term. There is compelling evidence that the different sexes of passengers responded different to their class status in terms of their likely survival. There appears to be a rough halving of survival between the 1st and 3rd classes for both sexes but in woman 2nd class females were not far behind their first class counterparts in probability of survival.</p>

<h2 id="tree-based-methods">Tree based methods</h2>

<p>We use a decision tree approach to build the set of splitting rules used to segment the predictor space. This is to improve our interpretation of the logistic regression model which can be difficult due to the the use of a non-intuitive but mathematically convenient logit link function. By providing  a simple and useful interpretation it may assist us in deciding how to proceed in order to improve our prediction accuracy.</p>

<p>{% highlight r %}
hist(train$Fare, col = &ldquo;grey&rdquo;, main = &ldquo;&rdquo;, xlab = &ldquo;Fare&rdquo;)
{% endhighlight %}</p>

<p><img src="/figures/2016-05-21_hist1-1.svg" alt="plot of chunk 2016-05-21_hist1" /></p>

<p>We go ahead and fit a decision tree after modifying the data slightly. We specify the qualitative variables as factors, ordered if appropriate. The new variables <code>Fare</code> and <code>SibSp</code> are included in the model. Presumably <code>Fare</code> is the amount paid for a ticket and <code>SibSp</code></p>

<p>{% highlight r %}
library(rpart)
library(rattle)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="warning-package-rattle-was-built-under-r-version-3-2-5">Warning: package &lsquo;rattle&rsquo; was built under R version 3.2.5</h2>

<p>{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="rattle-a-free-graphical-interface-for-data-mining-with-r">Rattle: A free graphical interface for data mining with R.</h2>

<h2 id="version-4-1-0-copyright-c-2006-2015-togaware-pty-ltd">Version 4.1.0 Copyright &copy; 2006-2015 Togaware Pty Ltd.</h2>

<h2 id="type-rattle-to-shake-rattle-and-roll-your-data">Type &lsquo;rattle()&rsquo; to shake, rattle, and roll your data.</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}</p>

<h2 id="let-s-modify-the-inputs-a-bit-to-help-our-ml-method-out">Let&rsquo;s modify the inputs a bit to help our ML method out</h2>

<h2 id="if-you-put-rubbish-in-you-get-rubbish-out">If you put rubbish in you get rubbish out</h2>

<p>dt_train1 &lt;- train %&gt;%
     dplyr::select(-PassengerId,-Name,-Ticket,-Cabin) %&gt;%
     dplyr::mutate(
          Survived = factor(Survived),
          Pclass = ordered(Pclass),
          SibSp = ordered(SibSp),
          Parch = ordered(Parch,levels = c(0:6,9))
     )</p>

<p>dt.train &lt;- rpart(
     Survived ~ Pclass + Sex + Fare + SibSp,
     dt_train1
     )</p>

<p>asRules(dt.train)  #  one rule for each path from root to leaf
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="rule-number-211-survived-1-cover-8-1-prob-1-00">Rule number: 211 [Survived=1 cover=8 (1%) prob=1.00]</h2>

<h2 id="sex-female">Sex=female</h2>

<h2 id="pclass-3">Pclass=3</h2>

<h2 id="fare-23-35">Fare&lt; 23.35</h2>

<h2 id="fare-7-888">Fare&gt;=7.888</h2>

<h2 id="fare-15-37">Fare&lt; 15.37</h2>

<h2 id="fare-13-91">Fare&lt; 13.91</h2>

<h2 id="fare-10-82">Fare&gt;=10.82</h2>

<h2 id="rule-number-7-survived-1-cover-170-19-prob-0-95">Rule number: 7 [Survived=1 cover=170 (19%) prob=0.95]</h2>

<h2 id="sex-female-1">Sex=female</h2>

<h2 id="pclass-1-2">Pclass=1,2</h2>

<h2 id="rule-number-27-survived-1-cover-42-5-prob-0-71">Rule number: 27 [Survived=1 cover=42 (5%) prob=0.71]</h2>

<h2 id="sex-female-2">Sex=female</h2>

<h2 id="pclass-3-1">Pclass=3</h2>

<h2 id="fare-23-35-1">Fare&lt; 23.35</h2>

<h2 id="fare-7-888-1">Fare&lt; 7.888</h2>

<h2 id="rule-number-53-survived-1-cover-30-3-prob-0-70">Rule number: 53 [Survived=1 cover=30 (3%) prob=0.70]</h2>

<h2 id="sex-female-3">Sex=female</h2>

<h2 id="pclass-3-2">Pclass=3</h2>

<h2 id="fare-23-35-2">Fare&lt; 23.35</h2>

<h2 id="fare-7-888-2">Fare&gt;=7.888</h2>

<h2 id="fare-15-37-1">Fare&gt;=15.37</h2>

<h2 id="rule-number-210-survived-0-cover-25-3-prob-0-32">Rule number: 210 [Survived=0 cover=25 (3%) prob=0.32]</h2>

<h2 id="sex-female-4">Sex=female</h2>

<h2 id="pclass-3-3">Pclass=3</h2>

<h2 id="fare-23-35-3">Fare&lt; 23.35</h2>

<h2 id="fare-7-888-3">Fare&gt;=7.888</h2>

<h2 id="fare-15-37-2">Fare&lt; 15.37</h2>

<h2 id="fare-13-91-1">Fare&lt; 13.91</h2>

<h2 id="fare-10-82-1">Fare&lt; 10.82</h2>

<h2 id="rule-number-2-survived-0-cover-577-65-prob-0-19">Rule number: 2 [Survived=0 cover=577 (65%) prob=0.19]</h2>

<h2 id="sex-male">Sex=male</h2>

<h2 id="rule-number-104-survived-0-cover-12-1-prob-0-17">Rule number: 104 [Survived=0 cover=12 (1%) prob=0.17]</h2>

<h2 id="sex-female-5">Sex=female</h2>

<h2 id="pclass-3-4">Pclass=3</h2>

<h2 id="fare-23-35-4">Fare&lt; 23.35</h2>

<h2 id="fare-7-888-4">Fare&gt;=7.888</h2>

<h2 id="fare-15-37-3">Fare&lt; 15.37</h2>

<h2 id="fare-13-91-2">Fare&gt;=13.91</h2>

<h2 id="rule-number-12-survived-0-cover-27-3-prob-0-11">Rule number: 12 [Survived=0 cover=27 (3%) prob=0.11]</h2>

<h2 id="sex-female-6">Sex=female</h2>

<h2 id="pclass-3-5">Pclass=3</h2>

<h2 id="fare-23-35-5">Fare&gt;=23.35</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
fancyRpartPlot(dt.train, palettes = c(&ldquo;Greys&rdquo;, &ldquo;Oranges&rdquo;))  #  grey is death, passengers with a bright future are orange
{% endhighlight %}</p>

<p><img src="/figures/2016-05-21_tree1-1.svg" alt="plot of chunk 2016-05-21_tree1" /></p>

<p>The root node, at the top, reveals that 62% of passengers died, while 38% survived. The number above these proportions indicates the way that the node is voting (at this top level it defaults to everyone will die, or coded as zero or coloured grey) the number at the bottom of the node indicates the proportion of the population that resides in this node, or bucket (here at the top level it is everyone, 100%).</p>

<p>All the men are assigned to the second node (labelled 2). This is a bucket of death where we assign all males to die, even though only 81% of the men died. More men were onboard and thus this bucket contains 65% of the total passengers (of the training data).</p>

<p>This decision tree provides quick rules of thumb for whether your future was bright (orange) or grey (dead). If you were a woman in the lowest class you chances were reduced to your wealthier classes. However if you paid a high end fare your chances improved despite being in the lowest class. Such intricacies and low number of samples in some of the leaves or buckets or nodes near the bottom suggest we may be in danger of overfitting.</p>

<p>Let&rsquo;s make some predictions from this simple tree and see if we built our simple <code>Sex</code> model earlier. We get our test data in a similar format as our training data, with appropriate factor classes. We then use <code>predict()</code> to predict whether the test passengers are likely to survive given their characteristics. We then write this to a file ready for submission!</p>

<p>{% highlight r %}
dt_test &lt;- test %&gt;%
     dplyr::select(-PassengerId,-Name,-Ticket,-Cabin) %&gt;%
     dplyr::mutate(
          Pclass = ordered(Pclass),
          SibSp = ordered(SibSp),
          Parch = ordered(Parch,levels = c(0:6,9))
     )</p>

<p>dt_pred &lt;- predict(dt.train, dt_test, type = &ldquo;class&rdquo;)  #  feed in test data to the model and predict whether Survived or died
dt_submit &lt;- data.frame(PassengerId = test$PassengerId, Survived = dt_pred)
#write.csv(dt_submit, file = &ldquo;submit_dtree.csv&rdquo;, row.names = FALSE)
{% endhighlight %}</p>

<p>An improvement of 0.02392% accuracy, this moved us up the leaderboard almost 2000 places.</p>

<h2 id="pruning-the-tree">Pruning the tree</h2>

<p>Inspecting the tree suggested it might be overlycomplex. We might want to prune the tree by recursively snipping off the least important splits based on the complexity parameter (I used trial and error for this value after reading the help file).</p>

<p>{% highlight r %}
dt.train2 &lt;- prune(dt.train, cp = 0.012)</p>

<p>asRules(dt.train2)  #  one rule for each path from root to leaf
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="rule-number-7-survived-1-cover-170-19-prob-0-95-1">Rule number: 7 [Survived=1 cover=170 (19%) prob=0.95]</h2>

<h2 id="sex-female-7">Sex=female</h2>

<h2 id="pclass-1-2-1">Pclass=1,2</h2>

<h2 id="rule-number-27-survived-1-cover-42-5-prob-0-71-1">Rule number: 27 [Survived=1 cover=42 (5%) prob=0.71]</h2>

<h2 id="sex-female-8">Sex=female</h2>

<h2 id="pclass-3-6">Pclass=3</h2>

<h2 id="fare-23-35-6">Fare&lt; 23.35</h2>

<h2 id="fare-7-888-5">Fare&lt; 7.888</h2>

<h2 id="rule-number-53-survived-1-cover-30-3-prob-0-70-1">Rule number: 53 [Survived=1 cover=30 (3%) prob=0.70]</h2>

<h2 id="sex-female-9">Sex=female</h2>

<h2 id="pclass-3-7">Pclass=3</h2>

<h2 id="fare-23-35-7">Fare&lt; 23.35</h2>

<h2 id="fare-7-888-6">Fare&gt;=7.888</h2>

<h2 id="fare-15-37-4">Fare&gt;=15.37</h2>

<h2 id="rule-number-52-survived-0-cover-45-5-prob-0-40">Rule number: 52 [Survived=0 cover=45 (5%) prob=0.40]</h2>

<h2 id="sex-female-10">Sex=female</h2>

<h2 id="pclass-3-8">Pclass=3</h2>

<h2 id="fare-23-35-8">Fare&lt; 23.35</h2>

<h2 id="fare-7-888-7">Fare&gt;=7.888</h2>

<h2 id="fare-15-37-5">Fare&lt; 15.37</h2>

<h2 id="rule-number-2-survived-0-cover-577-65-prob-0-19-1">Rule number: 2 [Survived=0 cover=577 (65%) prob=0.19]</h2>

<h2 id="sex-male-1">Sex=male</h2>

<h2 id="rule-number-12-survived-0-cover-27-3-prob-0-11-1">Rule number: 12 [Survived=0 cover=27 (3%) prob=0.11]</h2>

<h2 id="sex-female-11">Sex=female</h2>

<h2 id="pclass-3-9">Pclass=3</h2>

<h2 id="fare-23-35-9">Fare&gt;=23.35</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
fancyRpartPlot(dt.train2, palettes = c(&ldquo;Greys&rdquo;, &ldquo;Oranges&rdquo;))  #  grey is death, passengers with a bright future are orange
{% endhighlight %}</p>

<p><img src="/figures/2016-05-21_tree2-1.svg" alt="plot of chunk 2016-05-21_tree2" /></p>

<p>Now we compare how this does to our previous models. Did the reduction in complexity avoid overfitting and a reduction in test error or are we now missing out on modelling genuine complexity or patterns in the data?</p>

<p>{% highlight r %}
dt_pred &lt;- predict(dt.train2, dt_test, type = &ldquo;class&rdquo;)
dt_submit &lt;- data.frame(PassengerId = test$PassengerId, Survived = dt_pred)
#write.csv(dt_submit, file = &ldquo;submit_dtree2.csv&rdquo;, row.names = FALSE)
{% endhighlight %}</p>

<p>This tweaking caused no change in accuracy and failed to improve on our original decision tree! However, we did manage to prune a bunch of nodes or leaves if you look and compare the two trees. These extra nodes and branches were not useful for making predictions adding unneccessary complication to the model. Although we did not reduce our test error we improved our understanding, with the data suggesting that within the third class there may have been another strata of sub-class which influenced your chance of survival as a female. The wealthier women in the third class who paid a larger <code>Fare</code> were less likely to die.</p>

<h2 id="kaggler">KaggleR</h2>

<p>This is what Kaggle is all about brute forcing the solution to a problem by getting hundreds of top minds (teams or minds) interacting with a computer to try a vast combination of machine learning methods and parameter settings, all trying to work their way up the leaderboard. This combined effort will usually result in a much better solution to a problem than an in-house team good generate for a predictive problem. The gameification is also satisfying for the players and an interesting alternative for learning predictive analytics with machine learning as a compliment to the traditional textbook or online course.</p>

<p>{% highlight r %}
sessionInfo()
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="r-version-3-2-3-2015-12-10">R version 3.2.3 (2015-12-10)</h2>

<h2 id="platform-x86-64-w64-mingw32-x64-64-bit">Platform: x86_64-w64-mingw32/x64 (64-bit)</h2>

<h2 id="running-under-windows-8-x64-build-9200">Running under: Windows &gt;= 8 x64 (build 9200)</h2>

<h2 id="locale">locale:</h2>

<h2 id="1-lc-collate-english-united-kingdom-1252">[1] LC_COLLATE=English_United Kingdom.1252</h2>

<h2 id="2-lc-ctype-english-united-kingdom-1252">[2] LC_CTYPE=English_United Kingdom.1252</h2>

<h2 id="3-lc-monetary-english-united-kingdom-1252">[3] LC_MONETARY=English_United Kingdom.1252</h2>

<h2 id="4-lc-numeric-c">[4] LC_NUMERIC=C</h2>

<h2 id="5-lc-time-english-united-kingdom-1252">[5] LC_TIME=English_United Kingdom.1252</h2>

<h2 id="attached-base-packages">attached base packages:</h2>

<h2 id="1-grid-stats-graphics-grdevices-utils-datasets-methods">[1] grid      stats     graphics  grDevices utils     datasets  methods</h2>

<h2 id="8-base">[8] base</h2>

<h2 id="other-attached-packages">other attached packages:</h2>

<h2 id="1-rattle-4-1-0-rpart-4-1-10-rcolorbrewer-1-1-2">[1] rattle_4.1.0       rpart_4.1-10       RColorBrewer_1.1-2</h2>

<h2 id="4-vcd-1-4-1-purrr-0-2-1-ggplot2-2-0-0">[4] vcd_1.4-1          purrr_0.2.1        ggplot2_2.0.0</h2>

<h2 id="7-broom-0-4-0-magrittr-1-5-dplyr-0-4-3">[7] broom_0.4.0        magrittr_1.5       dplyr_0.4.3</h2>

<h2 id="10-testthat-0-11-0-knitr-1-12">[10] testthat_0.11.0    knitr_1.12</h2>

<h2 id="loaded-via-a-namespace-and-not-attached">loaded via a namespace (and not attached):</h2>

<h2 id="1-rcpp-0-12-4-formatr-1-2-1-plyr-1-8-3">[1] Rcpp_0.12.4        formatR_1.2.1      plyr_1.8.3</h2>

<h2 id="4-tools-3-2-3-digest-0-6-9-evaluate-0-8">[4] tools_3.2.3        digest_0.6.9       evaluate_0.8</h2>

<h2 id="7-memoise-0-2-1-gtable-0-2-0-nlme-3-1-122">[7] memoise_0.2.1      gtable_0.2.0       nlme_3.1-122</h2>

<h2 id="10-lattice-0-20-33-psych-1-5-8-dbi-0-4-1">[10] lattice_0.20-33    psych_1.5.8        DBI_0.4-1</h2>

<h2 id="13-parallel-3-2-3-rgtk2-2-20-31-stringr-1-0-0">[13] parallel_3.2.3     RGtk2_2.20.31      stringr_1.0.0</h2>

<h2 id="16-lmtest-0-9-34-r6-2-1-2-tidyr-0-4-0">[16] lmtest_0.9-34      R6_2.1.2           tidyr_0.4.0</h2>

<h2 id="19-reshape2-1-4-1-scales-0-4-0-mass-7-3-45">[19] reshape2_1.4.1     scales_0.4.0       MASS_7.3-45</h2>

<h2 id="22-rpart-plot-1-5-3-rsconnect-0-4-1-11-assertthat-0-1">[22] rpart.plot_1.5.3   rsconnect_0.4.1.11 assertthat_0.1</h2>

<h2 id="25-mnormt-1-5-4-colorspace-1-2-6-labeling-0-3">[25] mnormt_1.5-4       colorspace_1.2-6   labeling_0.3</h2>

<h2 id="28-stringi-1-0-1-lazyeval-0-1-10-munsell-0-4-3">[28] stringi_1.0-1      lazyeval_0.1.10    munsell_0.4.3</h2>

<h2 id="31-crayon-1-3-1-zoo-1-7-12">[31] crayon_1.3.1       zoo_1.7-12</h2>

<p>{% endhighlight %}</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

