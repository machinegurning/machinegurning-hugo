<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.35" />


<title>Nearest neighbour methods - A Hugo website</title>
<meta property="og:title" content="Nearest neighbour methods - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">8 min read</span>
    

    <h1 class="article-title">Nearest neighbour methods</h1>

    
    <span class="article-date">2016/05/27</span>
    

    <div class="article-content">
      

<p>{% include _toc.html %}</p>

<p>In my <a href="../least_squares_and_nearest_neighbours/">last post</a>, I started working through some examples given by Hastie et al in Elements of Statistical learning.
I looked at using a linear model for classification across a randomly generated training set.
In this post I&rsquo;ll use nearest neighbour methods to create a non-linear decision boundary over the same data.</p>

<h2 id="nearest-neighbour-algorithm">Nearest neighbour algorithm</h2>

<p>There are much more learned folk than I who give good explanations of the maths behind nearest neighbours, so I won&rsquo;t spend too long on the theory.
Hastie et al define the nearest neighbour approach as:</p>

<p>$$
\hat{Y}(x)=\frac{1}{k}\sum_{x_i\in N_k(x)}y_i
$$</p>

<p>The $k$ refers to the number of groups that we are interested in, and is user defined.
Our prediction $\hat{Y}$ (which is derived from $x$) is equal to the mean of $N_k$, where $N_k$ consists of the $k$ nearest training examples closest to $x$.</p>

<p>How do we define this closeness? Hastie et al simply use Euclidean distance:</p>

<p>$$
N<em>k(x) = \sqrt{\sum</em>{i=1}^n(x_i - x)^2\}
$$</p>

<p>So, simply put, all we need to do is look at the neighbours of a particular training example, and take an average of them, to create our prediction of the score of a given point.</p>

<h2 id="r-walkthrough">R Walkthrough</h2>

<p>As ever, the code to produce this post is available on github, <a href="https://github.com/machinegurning/machinegurning.github.io/tree/master/_rmd">here</a>.</p>

<p>Using the data I generated in my previous <a href="../least_squares_and_nearest_neighbours/">post</a> I&rsquo;ll walk through the process of producing a nearest neighbour prediction.</p>

<p>Just to recap: this is a dataset with 300 training examples, two features ($x_1$ and $x_2$), and a binary coded response variable ($X\in\mathbb{R}^{300 \times 2}$, $y\in{0,1}$):</p>

<p>{% highlight text %}</p>

<h2 id="source-local-data-frame-300-x-2">Source: local data frame [300 x 2]</h2>

<h2 id="x1-x2">x1          x2</h2>

<h2 id="dbl-dbl">(dbl)       (dbl)</h2>

<h2 id="1-0-47116692-1-01644895">1   0.47116692  1.01644895</h2>

<h2 id="2-0-24768010-1-53593828">2   0.24768010  1.53593828</h2>

<h2 id="3-0-16345097-0-71781636">3  -0.16345097  0.71781636</h2>

<h2 id="4-0-03240535-0-52703182">4  -0.03240535  0.52703182</h2>

<h2 id="5-0-54332650-0-75697322">5  -0.54332650  0.75697322</h2>

<h2 id="6-0-99554720-1-48845272">6   0.99554720  1.48845272</h2>

<h2 id="7-0-32698713-0-02188807">7   0.32698713 -0.02188807</h2>

<h2 id="8-0-53111911-1-29847189">8   0.53111911  1.29847189</h2>

<h2 id="9-0-29107906-1-17646395">9  -0.29107906  1.17646395</h2>

<h2 id="10-0-22141509-1-34857640">10  0.22141509  1.34857640</h2>

<h2 id="toc_15">..         &hellip;         &hellip;</h2>

<p>{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-1-1-1-1-1-1-1-1-1-1">[1] 1 1 1 1 1 1 1 1 1 1</h2>

<h2 id="levels-0-1">Levels: 0 1</h2>

<p>{% endhighlight %}</p>

<h3 id="calculating-euclidean-distance">Calculating Euclidean distance</h3>

<p>The first thing I need to do is calculate the Euclidean distance from every training example to every other training example - i.e. create a distance matrix. Fortunately R does this very simply with the <code>dist()</code> command.
This returns a $m \times m$ dimensional matrix</p>

<p>{% highlight r %}</p>

<h2 id="note-that-this-creates-a-dist-object-and-needs-to-be-coerced-into-a-vector">Note that this creates a dist object, and needs to be coerced into a vector.</h2>

<p>dist_matrix &lt;- X %&gt;%
  dist %&gt;%
  as.matrix</p>

<h2 id="for-brevity-i-won-t-print-it-instead-check-the-dimensions">For brevity, I won&rsquo;t print it. Instead check the dimensions</h2>

<p>dist_matrix %&gt;% dim
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-300-300">[1] 300 300</h2>

<p>{% endhighlight %}</p>

<p>I&rsquo;m interested in the 15 nearest neighbour average, like Hastie et al., so I just need need to extract the 15 shortest distances from each of these columns.
It helps at this point to break the matrix into a list using <code>split()</code>, with a vector element where each column was.
This will allow me to use <code>purrr::map()</code> which has an easier syntax than other loop handlers like <code>apply()</code> and its cousins.</p>

<p>{% highlight r %}</p>

<h2 id="use-split-to-convert-the-matrix-into-a-list">Use split to convert the matrix into a list</h2>

<p>dist_matrix_split &lt;-  dist_matrix %&gt;%
  split(
    f = rep(1:ncol(dist_matrix), each = nrow(dist_matrix))
  )</p>

<h2 id="check-that-we-have-a-list-of-length-m">Check that we have a list of length m</h2>

<p>dist_matrix_split %&gt;% class
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-list">[1] &ldquo;list&rdquo;</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
dist_matrix_split %&gt;% length
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-300">[1] 300</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}</p>

<h2 id="what-about-a-single-element-of-the-list">What about a single element of the list?</h2>

<p>dist_matrix_split %&gt;% extract2(1) %&gt;% class
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-numeric">[1] &ldquo;numeric&rdquo;</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
dist_matrix_split %&gt;% extract2(1) %&gt;% length
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-300-1">[1] 300</h2>

<p>{% endhighlight %}</p>

<p>Now I need a small helper function to return the closest $k$ points, so that I can take an average.
For this I use <code>order()</code></p>

<p>{% highlight r %}
return_k &lt;- function(x,k) {</p>

<p>order(x)[1:k] %&gt;%
    return
}
{% endhighlight %}</p>

<p>This should return a vector element in the list containing the index of $D$ which corresponds to the $k$ closest training examples.</p>

<p>{% highlight r %}
ranks &lt;- dist_matrix_split %&gt;%
  map(return_k, k = 15)</p>

<p>ranks[1:2]
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1">$<code>1</code></h2>

<h2 id="1-1-28-206-24-196-104-12-70-203-8-193-27-26-188-185">[1]   1  28 206  24 196 104  12  70 203   8 193  27  26 188 185</h2>

<h2 id="2">$<code>2</code></h2>

<h2 id="1-2-90-29-10-16-114-17-8-15-26-18-24-182-118-28">[1]   2  90  29  10  16 114  17   8  15  26  18  24 182 118  28</h2>

<p>{% endhighlight %}</p>

<p>So far so good, the function returns us 15 indices with which we can subset $y$ to get our 15 nearest neighbour majority vote.
The values of $y$ are then averaged&hellip;</p>

<p>{% highlight r %}</p>

<h1 id="note-i-coerce-to-character-first-then-to-integer-otherwise-our-integers-are">Note I coerce to character first, then to integer, otherwise our integers are</h1>

<h1 id="not-zero-indexed">not zero indexed.</h1>

<p>y_hat &lt;- ranks %&gt;%
  map(function(x) y[x]) %&gt;%
  map_dbl(function(x) x %&gt;% as.character %&gt;% as.integer %&gt;% mean)</p>

<p>y_hat[1:10]
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-2-3-4-5-6-7">1         2         3         4         5         6         7</h2>

<h2 id="0-9333333-0-9333333-0-5333333-0-4000000-0-4000000-1-0000000-0-4666667">0.9333333 0.9333333 0.5333333 0.4000000 0.4000000 1.0000000 0.4666667</h2>

<h2 id="8-9-10">8         9        10</h2>

<h2 id="0-9333333-0-7333333-0-8666667">0.9333333 0.7333333 0.8666667</h2>

<p>{% endhighlight %}</p>

<p>&hellip;and converted into a binary classification, such that $G\in{0,1}$: where $\hat{Y}&gt;0.5$, $G=1$, otherwise $G=0$.</p>

<p>{% highlight r %}
G &lt;- ifelse(y_hat &gt;= 0.5, 1, 0)</p>

<p>G[1:10]
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-2-3-4-5-6-7-8-9-10">1  2  3  4  5  6  7  8  9 10</h2>

<h2 id="1-1-1-0-0-1-0-1-1-1">1  1  1  0  0  1  0  1  1  1</h2>

<p>{% endhighlight %}</p>

<h3 id="intuition">Intuition</h3>

<p>Before looking at the predictions, now is a good point for a quick recap on what the model is actually doing.</p>

<p>For the training examples $(10, 47, 120)$ I have run the code above, and plotted out the 15 nearest neighbours whose $y$ is averaged to get out prediction $G$.</p>

<p>For the right hand point you can see that for all of the 15 nearest neighbours $y=1$, hence for our binary prediction $G=1$.
The opposite can be said for the left hand: again there is a unanimous vote, and so $G=0$.
For the middle point, most of the time $y=1$, hence although there is not unanimity, our prediction for this point would be $G=1$.</p>

<p>You can image that from this plot: whilst varying $k$ would have little effect on the points that are firmly within the respective classes, points close to the decision boundary are likely to be affected by small changes in $k$.
Set $k$ too low, and we invite <em>bias</em>, set $k$ too high, and we are likely to increase <em>variance</em>.
I&rsquo;ll come back to this.</p>

<p><img src="/figures/2016-05-27-intuition-1.svg" alt="plot of chunk 2016-05-27-intuition" /></p>

<h3 id="predictions">Predictions</h3>

<p>So how do the predictions made by nearest neighbours ($k=15$) match up with the actual values of $y$ in this training set?</p>

<p>{% highlight r %}
table(y,G)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="g">G</h2>

<h2 id="y-0-1">y     0   1</h2>

<h2 id="0-114-6">0 114   6</h2>

<h2 id="1-4-176">1   4 176</h2>

<p>{% endhighlight %}</p>

<p>In general: pretty good, and marginally better than the linear classifier I used in the previous [post]().
In just $3$% of cases does our classifier get it wrong.</p>

<h3 id="decision-boundary">Decision boundary</h3>

<p>For this next plot, I use the <code>class::knn()</code> function to replace the long-winded code I produced earlier.
This function allows us to train our classifier on a training set, and then apply it to a test set, all in one simple function.</p>

<p>In this case I produce a test set which is just a grid of points. By applying the model to this data, I can produce a decision boundary which can be plotted.</p>

<p>{% highlight r %}</p>

<h2 id="generate-a-grid-of-points">Generate a grid of points</h2>

<p>test_grid = seq(-3.5, 3.5, length = 800)</p>

<p>X_test &lt;- data.frame(
  x1 = rep(test_grid, times = length(test_grid)),
  x2 = rep(test_grid, each = length(test_grid))
)</p>

<h1 id="run-knn-on-our-training-set-x-and-output-predictions-into-a-dataframe-alongside-x-test">Run knn on our training set X and output predictions into a dataframe alongside X_test.</h1>

<p>knn_pred &lt;- class::knn(
  train = X,
  test = X_test,
  cl = y,
  k = 15
) %&gt;%
  data.frame(
    X_test,
    pred = .
  )</p>

<h1 id="now-plot-using-geom-contour-to-draw-the-decision-boundary">Now plot,using geom_contour to draw the decision boundary.</h1>

<p>knn_pred %&gt;%
  ggplot +
  aes(
    x = x1,
    y = x2,
    colour = y
  ) +
  geom_point(
    data = D2,
    aes(
      colour = actual,
      shape = prediction
    ),
    size = 3
  ) +
  geom_contour(
    aes(
      z = as.integer(pred)
    ),
    size = 0.4,
    colour = &ldquo;black&rdquo;,
    bins = 1
  ) +
  coord_cartesian(
    xlim = c(-2.2,2.8),
    ylim = c(-3,3.5)
  ) +
  xlab(&ldquo;X&rdquo;) +
  ylab(&ldquo;Y&rdquo;)
{% endhighlight %}</p>

<p><img src="/figures/2016-05-27-decision_boundary-1.svg" alt="plot of chunk 2016-05-27-decision_boundary" /></p>

<h3 id="varying-k">Varying k</h3>

<p>I mentioned before the impact that varying $k$ might have.
Here I have run <code>knn()</code> on the same data but for multiple values of $k$.
For $k=1$ we get a perfect fit with multiple polygons separating all points in each class perfectly.
As $k$ increases, we see that the more peripheral polygons start to break down, until at $k=15$ there is largely a singular decision boundary which weaves its way between the two classes.
At $k=99$, this decision boundary is much more linear.</p>

<p><img src="/figures/2016-05-27-varying-k-1.svg" alt="plot of chunk 2016-05-27-varying-k" /></p>

<p>In my next post I will address this problem of setting $k$ again, and try to quantify when the model is suffering from variance or bias.</p>

<p>{% highlight r %}
sessionInfo()
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="r-version-3-3-0-2016-05-03">R version 3.3.0 (2016-05-03)</h2>

<h2 id="platform-x86-64-pc-linux-gnu-64-bit">Platform: x86_64-pc-linux-gnu (64-bit)</h2>

<h2 id="running-under-ubuntu-14-04-4-lts">Running under: Ubuntu 14.04.4 LTS</h2>

<h2 id="locale">locale:</h2>

<h2 id="1-lc-ctype-en-gb-utf-8-lc-numeric-c">[1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C</h2>

<h2 id="3-lc-time-en-gb-utf-8-lc-collate-en-gb-utf-8">[3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8</h2>

<h2 id="5-lc-monetary-en-gb-utf-8-lc-messages-en-gb-utf-8">[5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8</h2>

<h2 id="7-lc-paper-en-gb-utf-8-lc-name-c">[7] LC_PAPER=en_GB.UTF-8       LC_NAME=C</h2>

<h2 id="9-lc-address-c-lc-telephone-c">[9] LC_ADDRESS=C               LC_TELEPHONE=C</h2>

<h2 id="11-lc-measurement-en-gb-utf-8-lc-identification-c">[11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C</h2>

<h2 id="attached-base-packages">attached base packages:</h2>

<h2 id="1-stats-graphics-grdevices-utils-datasets-base">[1] stats     graphics  grDevices utils     datasets  base</h2>

<h2 id="other-attached-packages">other attached packages:</h2>

<h2 id="1-tidyr-0-4-1-ggplot2-2-1-0-dplyr-0-4-3-magrittr-1-5">[1] tidyr_0.4.1    ggplot2_2.1.0  dplyr_0.4.3    magrittr_1.5</h2>

<h2 id="5-tibble-1-0-purrr-0-2-1-testthat-0-8-1-knitr-1-12-3">[5] tibble_1.0     purrr_0.2.1    testthat_0.8.1 knitr_1.12.3</h2>

<h2 id="loaded-via-a-namespace-and-not-attached">loaded via a namespace (and not attached):</h2>

<h2 id="1-rcpp-0-12-4-class-7-3-14-digest-0-6-4-assertthat-0-1">[1] Rcpp_0.12.4      class_7.3-14     digest_0.6.4     assertthat_0.1</h2>

<h2 id="5-plyr-1-8-3-grid-3-3-0-r6-2-1-2-gtable-0-2-0">[5] plyr_1.8.3       grid_3.3.0       R6_2.1.2         gtable_0.2.0</h2>

<h2 id="9-dbi-0-4-formatr-1-3-scales-0-4-0-evaluate-0-9">[9] DBI_0.4          formatR_1.3      scales_0.4.0     evaluate_0.9</h2>

<h2 id="13-lazyeval-0-1-10-labeling-0-3-tools-3-3-0-stringr-0-6-2">[13] lazyeval_0.1.10  labeling_0.3     tools_3.3.0      stringr_0.6.2</h2>

<h2 id="17-munsell-0-4-3-parallel-3-3-0-colorspace-1-2-6-methods-3-3-0">[17] munsell_0.4.3    parallel_3.3.0   colorspace_1.2-6 methods_3.3.0</h2>

<p>{% endhighlight %}</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

