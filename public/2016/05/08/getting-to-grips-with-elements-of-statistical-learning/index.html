<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.35" />


<title>Getting to grips with &#39;Elements of statistical learning&#39; - A Hugo website</title>
<meta property="og:title" content="Getting to grips with &#39;Elements of statistical learning&#39; - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">13 min read</span>
    

    <h1 class="article-title">Getting to grips with &#39;Elements of statistical learning&#39;</h1>

    
    <span class="article-date">2016/05/08</span>
    

    <div class="article-content">
      

<p>{% include _toc.html %}</p>

<p>Last week I joined a reading group for the weighty tome <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">Elements of Statistical Learning</a>.
I really like the idea of this group; interesting as it is - it can be hard to maintain the drive to wade through a text like this.
Working through it week on week with a group of like-minded people is a great way to overcome this.</p>

<h1 id="linear-models">Linear models</h1>

<p>In this post I implement in R some of the ideas that are presented in the first 2 chapters of Elements of Statistical Learning, namely: least squares.
I&rsquo;ve written about linear models before; whilst doing Andrew Ng&rsquo;s excellent <a href="https://www.coursera.org/learn/machine-learning/home/info?source=cdpv2">Machine learning</a> I wrote a toy R package <a href="https://github.com/ivyleavedtoadflax/vlrr">vlrr</a> to implement linear regression with regularisation mainly as an exercise in ackage development.</p>

<h2 id="prediction-from-linear-models">Prediction from linear models</h2>

<p>Linear models are the bread and butter of statistics.
They are pretty fundamental, and I don&rsquo;t want to write too much about them here for that reason.</p>

<p>However, one thing I like about the way that Hastie et al. present the subject is in terms of matrix operations, giving a vectorized implementation that can easily be translated into code.</p>

<p>In simple statistics, we are used to seeing linear models represeted as:</p>

<p>$$y = bx + a$$</p>

<p>Where our prediction $y$ is dependent on a rate or slope $b$, and a constant or intercept $a$. Simple.</p>

<p>Hastie, et al. present the more general form of this equation<sup class="footnote-ref" id="fnref:1"><a rel="footnote" href="#fn:1">1</a></sup>:</p>

<p>$$
\hat{Y}=\hat\beta<em>0+\sum^{m}</em>{j=1}X_j\hat\beta_j\cdot
$$</p>

<p>Here, the prediction $Y$ is given by the addition of the intercept (or bias) $\beta<em>0$ and the sum of the dot product of $X</em>{1..m}$ and $\beta_{1..m}$ where $X$ is an $n$ by $m$ dimensional matrix ($X\in\mathbb{R}^{n \times m}$), and $\beta$ is an $n$ dimensional vector (or later as we shall see: $\beta\in\mathbb{R}^{k \times n}$ - where $k$ is the number of models we wish to apply).</p>

<p>By including a constant of 1 as the first column vector of the matrix $X$, it is possible to greatly simplify this equation into an matrix inner product:</p>

<p>$$
Y = X^T \beta
$$</p>

<p>This can be a bit of a leap, so I break this down more simply here.
Let us create an input vector $\vec{x}$ where $\vec{x}\in\mathbb{R}^{m}$: i.e. there are $m$ training examples, and in this case: $m=10$.</p>

<p>{% highlight r %}</p>

<h1 id="load-dplyr-to-make-life-easier">Load dplyr to make life easier</h1>

<p>library(dplyr)</p>

<h1 id="set-random-seed-to-fix-the-answer">Set random seed to fix the answer</h1>

<p>set.seed(1337)</p>

<p>m &lt;- 10
x &lt;- runif(n = m, min = 0, max = 500) %&gt;%
  round</p>

<p>x
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-288-282-37-227-187-166-474-141-123-73"><a href="1" title="From this equation onwards, I drop Hastie et al's convention of denoting matrices with $\hat{}$.
">1</a> 288 282  37 227 187 166 474 141 123  73</h2>

<p>{% endhighlight %}</p>

<p>To get my prediction out of $\vec{x}$, I need to supply coefficients $a$ and $b$ (or $\beta$ if we use Hastie et al&rsquo;s notation; $\theta$ if we use Andrew Ng&rsquo;s).</p>

<p>In this case, I&rsquo;m going to use a very simple model by setting $a = 1$, and $b = 10$.
I&rsquo;m not worrying about how we get these coefficients here, and nor should you be - we&rsquo;re just interested in the predictions we are going to make using these our input $\vec{x}$ and our coefficients $\beta$</p>

<p>So this is how I calculate that prediction in R:</p>

<p>{% highlight r %}
a &lt;- 1
b &lt;- 10</p>

<p>y &lt;- (x * a) + b</p>

<p>y
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-298-292-47-237-197-176-484-151-133-83"><a href="1" title="From this equation onwards, I drop Hastie et al's convention of denoting matrices with $\hat{}$.
">1</a> 298 292  47 237 197 176 484 151 133  83</h2>

<p>{% endhighlight %}</p>

<p>Of course, R has vectorised the operation here, so every element of our vector $\vec{x}$ was multiplied by $a$.
We can express this as:</p>

<p>$$
\begin{align}
y_1 &amp;= 288 \times 1 + 10 \
y_2 &amp;= 282 \times 1 + 10 <br />
y<em>3 &amp;= 37 \times 1 + 10 <br />
\vdots <br />
y</em>{10} &amp;= 73 \times 1 + 10 <br />
\end{align}
$$</p>

<h2 id="thinking-in-matrices">Thinking in matrices</h2>

<p>So now lets think about this more explicitly in matrix terms.</p>

<h3 id="matrix-operations-recap">Matrix operations recap</h3>

<p>A quick reminder of matrix multiplication (for my benefit if nothing else).
To multiply a matrix $A\in\mathbb{R}^{3 \times 2}$ with matrix $B\in\mathbb{R}^{2 \times 3}$, we multiply $A$ with each of the columns of $B$ to give $C\in\mathbb{R}^{3 \times 3}$.
Remember that the number of columns of $A$ must equal the number of rows of $B$, e.g:</p>

<p>$$
A \times B = C <br />
\begin{bmatrix}
1 &amp; 3 <br />
2 &amp; 5 <br />
0 &amp; 9 <br />
\end{bmatrix}
\times
\begin{bmatrix}
3 &amp; 3 &amp; 2<br />
2 &amp; 5 &amp; 7<br />
\end{bmatrix}=
\begin{bmatrix}
9 &amp; 18 &amp; 23<br />
16 &amp; 31 &amp; 39<br />
18 &amp; 45 &amp; 63<br />
\end{bmatrix}
$$</p>

<p>So, for instance:</p>

<p>$$
\begin{align}
C<em>{3,1} &amp;=  (A</em>{3,1} \times B<em>{1,1}) + (A</em>{3,2} \times B<em>{2,1})<br />
C</em>{3,1} &amp;= (0 \times 3) + (9 \times 2) <br />
C_{3,1} &amp;= 18
\end{align}
$$</p>

<h3 id="linear-model-by-matrix-inner-product">Linear model by matrix inner product</h3>

<p>First we place our coefficients $a$ and $b$ into the column vector $\beta$, and add the constant 1 to the input vector $x$ to give the $n + 1$ (one input vector, and the constant 1) by $m$ (the number of training examples) matrix $X$.</p>

<p>So applying those quick recaps to our equation $Y=X^T\beta$, we get<sup class="footnote-ref" id="fnref:2"><a rel="footnote" href="#fn:2">2</a></sup>:</p>

<p>$$
\begin{bmatrix}
1 &amp; 288<br />
1 &amp; 282<br />
1 &amp; 37<br />
\vdots &amp; \vdots <br />
1 &amp; 73<br />
\end{bmatrix}
\cdot
\begin{bmatrix}
10 <br />
1 <br />
\end{bmatrix}=
\begin{bmatrix}
1 \times 10 + 288 \times 1<br />
1 \times 10 + 282 \times 1<br />
1 \times 10 + 37 \times 1<br />
\vdots <br />
1 \times 10 + 227 \times 1<br />
\end{bmatrix}=
\begin{bmatrix}
298<br />
292<br />
47<br />
\vdots <br />
237<br />
\end{bmatrix}
$$</p>

<p>In R we can do this simply with:</p>

<p>{% highlight r %}</p>

<h1 id="add-the-constant-1-as-x-0">Add the constant 1 as X_0</h1>

<p>X &lt;- matrix(
  cbind( 1, x ),
  ncol = 2
  )</p>

<h1 id="bind-a-and-b-into-a-vector-beta">Bind a and b into a vector beta</h1>

<p>beta &lt;- matrix(
  cbind( b, a ),
  nrow = 2
  )</p>

<p>Y &lt;- X %*% beta</p>

<p>Y
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1">[,1]</h2>

<h2 id="1-298">[1,]  298</h2>

<h2 id="2-292">[2,]  292</h2>

<h2 id="3-47">[3,]   47</h2>

<h2 id="4-237">[4,]  237</h2>

<h2 id="5-197">[5,]  197</h2>

<h2 id="6-176">[6,]  176</h2>

<h2 id="7-484">[7,]  484</h2>

<h2 id="8-151">[8,]  151</h2>

<h2 id="9-133">[9,]  133</h2>

<h2 id="10-83">[10,]   83</h2>

<p>{% endhighlight %}</p>

<p>We can check this against the earlier calculation of $y$:</p>

<p>{% highlight r %}</p>

<h1 id="need-to-subset-y-1-to-get-an-r-vector-back">Need to subset Y[,1] to get an R vector back</h1>

<p>identical(y,Y[,1])
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-true"><a href="1" title="From this equation onwards, I drop Hastie et al's convention of denoting matrices with $\hat{}$.
">1</a> TRUE</h2>

<p>{% endhighlight %}</p>

<h1 id="least-squares">Least squares</h1>

<p>Ok, this is all well and good, we can make a prediction $Y=X^T\beta$, but how on earth do we get $\beta$.
In previous posts, I variously used <a href="http://www.machinegurning.com/rstats/gradient-descent/">gradient descent</a>, the <a href="http://www.machinegurning.com/rstats/R_classes/">BFGS</a> algorithm, and the &lsquo;normal equation&rsquo; or least squares method, which is what I will reproduce here.</p>

<p>This method provides an exact solution for a given linear model, which is handy, but there are situations where this method may not be appropriate.
The main issue with the normal equation, is that when dealing with very large amounts of data i.e. $n&gt;10,000$ then the imperative to solve the matrix inverse $(X^TX)^{-1}$ means that it can be computationally expensive.
In addition, there are cases when the matrix given by $(X^TX)$ will not be invertible, and so will simply not work.
This typically occurs when feaures $X_i$ and $X_j$ are linearly dependent, or when there are too many input features, i.e. $X$ is wider than it is long, i.e. $p&gt;&gt;n$ problems.</p>

<p>To calculate $\beta$ we can simply solve the equations:</p>

<p>$$
RSS(\beta)=\sum^N_{i=1}(y_i-x^T_i\beta)^2
$$</p>

<p>This is the notation that Hastie, et al. use, and RSS stands for the residual sum of squares.
This simplifies (in matrix notation) to:</p>

<p>$$
\beta=(X^TX)^{-1}X^Ty
$$</p>

<p>In R, this looks like <code>solve(t(X) %*% X) %*% (t(X) %*% y)</code>, which should return <code>10 1</code>:</p>

<p>{% highlight r %}
coef_ne &lt;- solve(t(X) %<em>% X) %</em>% (t(X) %*% y)</p>

<p>coef_ne
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-1">[,1]</h2>

<h2 id="1-10">[1,]   10</h2>

<h2 id="2-1">[2,]    1</h2>

<p>{% endhighlight %}</p>

<h3 id="qr-decomposition">QR decomposition</h3>

<p>Actually it turns out that using this solution is not the most efficient.
Leisch<sup class="footnote-ref" id="fnref:3"><a rel="footnote" href="#fn:3">3</a></sup> counsels against it and instead, the base implemetation of <code>lm()</code> uses QR decomposition.</p>

<p>A quick Google, and you will see that QR decomposition has been considered to be one of the <a href="http://www.siam.org/pdf/news/637.pdf">most influential algorithms of the 20th Century</a>.
In simple terms<sup class="footnote-ref" id="fnref:4"><a rel="footnote" href="#fn:4">4</a></sup>, a QR decomposition is the breaking down of a matrix into two product matrices with specific properties.
If we start with a matrix $M$, QR decomposition would give us $M=QR$ where $Q$ is an orthogonal matrix, and $R$ an upper triangular matrix.</p>

<p>So for the matrix $X$ that we have been using so far, we can do this in R with the following:</p>

<p>{% highlight r %}</p>

<h1 id="first-create-a-qr-object">First create a QR object</h1>

<p>qrx &lt;- qr(X)</p>

<h1 id="calculate-q-and-r">Calculate Q and R</h1>

<p>Q &lt;- qr.Q(qrx, complete = TRUE)
R &lt;- qr.R(qrx)
{% endhighlight %}</p>

<p>This gives us:</p>

<p>$$
\begin{bmatrix}
1&amp;288 <br />
1&amp;282 <br />
1&amp;37 <br />
1&amp;227 <br />
1&amp;187 <br />
1&amp;166 <br />
1&amp;474 <br />
1&amp;141 <br />
1&amp;123 <br />
1&amp;73 <br />
\end{bmatrix}=
\begin{bmatrix}
-0.32&amp;-0.23&amp;-0.35&amp;-0.32&amp;-0.32&amp;-0.33&amp;-0.27&amp;-0.33&amp;-0.33&amp;-0.34 <br />
-0.32&amp;-0.22&amp;0.4&amp;-0.09&amp;0.01&amp;0.07&amp;-0.73&amp;0.13&amp;0.18&amp;0.31 <br />
-0.32&amp;0.43&amp;0.71&amp;-0.07&amp;-0.12&amp;-0.14&amp;0.21&amp;-0.17&amp;-0.19&amp;-0.25 <br />
-0.32&amp;-0.07&amp;-0.08&amp;0.92&amp;-0.08&amp;-0.08&amp;-0.07&amp;-0.08&amp;-0.08&amp;-0.08 <br />
-0.32&amp;0.03&amp;-0.12&amp;-0.07&amp;0.92&amp;-0.09&amp;-0.01&amp;-0.1&amp;-0.1&amp;-0.11 <br />
-0.32&amp;0.09&amp;-0.15&amp;-0.07&amp;-0.09&amp;0.9&amp;0.02&amp;-0.11&amp;-0.11&amp;-0.13 <br />
-0.32&amp;-0.72&amp;0.19&amp;-0.08&amp;-0.03&amp;0&amp;0.55&amp;0.04&amp;0.07&amp;0.14 <br />
-0.32&amp;0.16&amp;-0.17&amp;-0.07&amp;-0.09&amp;-0.1&amp;0.06&amp;0.88&amp;-0.13&amp;-0.15 <br />
-0.32&amp;0.2&amp;-0.19&amp;-0.07&amp;-0.1&amp;-0.11&amp;0.08&amp;-0.13&amp;0.86&amp;-0.17 <br />
-0.32&amp;0.33&amp;-0.25&amp;-0.07&amp;-0.11&amp;-0.13&amp;0.16&amp;-0.15&amp;-0.17&amp;0.79 <br />
\end{bmatrix}
\begin{bmatrix}
-3.16&amp;-631.82 <br />
0&amp;-379.09 <br />
\end{bmatrix}
$$</p>

<p>I&rsquo;m not going to go into any more detail here, but suffice it to say that the <code>qr</code> object can simply be solved in R to return our coefficients $\beta$:</p>

<p>{% highlight r %}</p>

<h1 id="if-we-first-created-the-qr-object">If we first created the qr object:</h1>

<p>solve.qr(qrx, y)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-10-1"><a href="1" title="From this equation onwards, I drop Hastie et al's convention of denoting matrices with $\hat{}$.
">1</a> 10  1</h2>

<p>{% endhighlight %}</p>

<p>The explanation for why QR decomposition is favoured over solving the normal equation rests in part in the expensive operation $(X^TX)^{-1}$.
In my experiments (which were admittedly not very scientific), the QR method seemed to take as little as half the time of least squares when trying to solve $X\in\mathbb{R}^{m \times n}$ for large matrices.
Furthermore, where $n$ is much larger than $m$ (say 10 times), the normal equation fails completely, and will return the following error in R:</p>

<p><code>system is computationally singular: reciprocal condition number</code></p>

<p>whilst the QR method will at least complete (see the underlying .Rmd for an example I tried).</p>

<h1 id="linear-models-for-classification">Linear models for classification</h1>

<p>So now we have seen how to get the parameters $\beta$, I will use a linear model in anger.
Here I reproduce the example by Hastie et al. to show a simple linear model used for two class classification.</p>

<h2 id="generate-some-data">Generate some data</h2>

<p>First we generate data based on two distinct normal distributions, which we will seek to separate usin gthe linear model.
I&rsquo;ve copied this code from my earlier post on <a href="/rstats/knn">k-means</a>.</p>

<p>In the code chunk below I use Hadley&rsquo;s excellent <a href="https://github.com/hadley/purrr">purrr</a> package to create 10 bivariate normal distributions, which are then plotted together.
The reason for this will become apparent when I move onto nearest neighbour methods in my next post.</p>

<p>{% highlight r %}</p>

<h1 id="first-generate-some-training-data">First generate some training data.</h1>

<p>library(purrr)
library(tibble)
library(magrittr)</p>

<h1 id="set-a-seed-for-reproducibility">Set a seed for reproducibility</h1>

<p>set.seed(1337)</p>

<h1 id="this-function-will-create-bivariate-normal-distributions-about-two-means-with">This function will create bivariate normal distributions about two means with</h1>

<h1 id="a-singular-deviation">a singular deviation</h1>

<p>dummy_group &lt;- function(
  x = 30,
  mean1 = 10,
  mean2 = 10,
  sd = 0.45
  ) {</p>

<p>cbind(
    rnorm(x, mean1, sd),
    rnorm(x, mean2, sd)
  )</p>

<p>}</p>

<h1 id="generate-10-bivariate-distributions-using-normal-distributions-to-generate-the">Generate 10 bivariate distributions using normal distributions to generate the</h1>

<h1 id="means-for-each-of-the-two-variables-bind-this-all-together-into-a-dataframe">means for each of the two variables. Bind this all together into a dataframe,</h1>

<h1 id="and-label-this-for-training-examples-note-that-i-draw-the-distinctions">and label this for training examples. Note that I draw the distinctions</h1>

<h1 id="between-0s-and-1s-pretty-much-by-eye-there-was-not-magic-to-this">between 0s and 1s, pretty much by eye - there was not magic to this.</h1>

<p>dummy_data &lt;- data_frame(
  mean1 = rnorm(10),
  mean2 = rnorm(10)
) %&gt;%
  pmap(dummy_group) %&gt;%
  map(as.data.frame) %&gt;%
  rbind_all %&gt;%
  mutate(
    group = rep(1:10, each = 30),
    group = factor(group),
    group_bit = ifelse(group %in% c(2,3,5,10), 0, 1),
    group_bit = factor(group_bit)
  ) %&gt;%
  select(
    X = V1,
    Y = V2,
    group,
    group_bit
    )
{% endhighlight %}</p>

<p>I set $G\in{0,1}$ (having divided the bivariate distributions roughly by eye); so now we can train a model based on $G$ to find a decision boundary.</p>

<p>Now lets plot the data to see what we have.</p>

<p>{% highlight r %}
library(ggplot2)</p>

<p>p &lt;- dummy_data %&gt;%
  ggplot +
  aes(
    x = X,
    y = Y,
    colour = group_bit
  ) +
  geom_point(
    size = 3
    )</p>

<p>p
{% endhighlight %}</p>

<p><img src="/figures/2016-05-08-linear-classification-1.svg" alt="plot of chunk 2016-05-08-linear-classification" /></p>

<h2 id="applying-the-linear-model">Applying the linear model</h2>

<p>Now lets try to apply a linear model to the data, using $G$ as the explanatory variable.</p>

<p>{% highlight r %}
#Start by defining training data - note that these must be vector/matrix, not
#dataframes.</p>

<p>G &lt;- dummy_data$group_bit %&gt;%
  #as.character %&gt;%
  as.integer</p>

<p>X &lt;- dummy_data[,c(&ldquo;X&rdquo;,&ldquo;Y&rdquo;)] %&gt;%
  as.matrix
{% endhighlight %}</p>

<p>For the sake of argument, I use both the normal equation for least squares, but also use the $QR$ decomposition method.</p>

<p>{% highlight r %}
beta &lt;- solve(t(X) %<em>% X) %</em>% (t(X) %*% G)
beta_qr &lt;- qr.solve(X, G)</p>

<p>beta
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-2">[,1]</h2>

<h2 id="x-0-4141801">X 0.4141801</h2>

<h2 id="y-0-4560902">Y 0.4560902</h2>

<p>{% endhighlight %}</p>

<p>And we can check that these match&hellip;</p>

<p>{% highlight r %}
all.equal(
  as.vector(beta),
  as.vector(beta_qr)
  )
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-true-1"><a href="1" title="From this equation onwards, I drop Hastie et al's convention of denoting matrices with $\hat{}$.
">1</a> TRUE</h2>

<p>{% endhighlight %}</p>

<p>Great.
So how does our model do?</p>

<p>{% highlight r %}</p>

<h1 id="make-our-prediction-with-x-b-and-round-this-to-get-full-numbers-any-values">Make our prediction with X&rsquo;B, and round this to get full numbers. Any values</h1>

<h1 id="above-0-become-1-below-zero-becomes-0-note-that-r-has-changed-g-from-g-in">above 0 become 1, below zero becomes 0. Note that R has changed G from G in</h1>

<h1 id="0-1-to-g-in-1-2-our-prediction-y-is-on-the-top-axis-the-actual-class-g">{0,1} to G in {1,2}. Our prediction Y is on the top axis, the actual class G</h1>

<h1 id="is-on-the-left-axis">is on the left axis.</h1>

<p>Y &lt;- X %*% beta
Y &lt;- ifelse(Y &gt; 0.5, 1, 0)</p>

<p>table(G, Y)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="y">Y</h2>

<h2 id="g-0-1">G     0   1</h2>

<h2 id="1-118-2">1 118   2</h2>

<h2 id="2-20-160">2  20 160</h2>

<p>{% endhighlight %}</p>

<p>So, most of the time, this very simple model is sufficient to make this binary classification.
Only in <sup>20</sup>&frasl;<sub>300</sub> cases do we get a Type II error (a false negative), whilst in <sup>2</sup>&frasl;<sub>300</sub> cases we get a Type I error (a false positive).</p>

<p>To plot the decision boundary, we need to create a grid of predictions which we can then divide by running the linear algorithm on.
The following function does this, then we can include the boundary on a plot with <code>geom_contour()</code>.</p>

<p>{% highlight r %}
draw_boundary &lt;- function(xy, beta) {</p>

<p>u &lt;- rep(xy, times = length(xy))
  v &lt;- rep(xy, each = length(xy))</p>

<p>X &lt;- cbind(u,v)</p>

<p>Y &lt;- X %*% beta</p>

<p>cbind(X, Y) %&gt;%
    as.data.frame %&gt;%
    mutate(
      actual = ifelse(Y &gt; 0.5, 1, 0)
    )
}
{% endhighlight %}</p>

<p>And, plotting it out using shapes to indicate the predictions, we can see that the decision boundary runs a little above the top of the actual $0$ class.
Anything above this line, our model has predicted to be $G=1$, and below it $G=0$.
The two pink triangles are our false positives, whilst the blue circles below the line are our false negatives.</p>

<p>{% highlight r %}
draw_boundary(
  seq(from = -4, to = 4, length = 1000),
  beta
  ) %&gt;%
  ggplot +
  aes(
    x = u,
    y = v,
    z = actual,
    colour = actual
  ) +
  geom_contour(
    size = 0.4,
    colour = &ldquo;black&rdquo;,
    bins = 1
  ) +
  geom_point(
    data = dummy_data %&gt;% cbind(prediction = Y) %&gt;%
      mutate(
        prediction = factor(prediction),
        actual = factor(group_bit)
      ),
    aes(
      x = X,
      y = Y,
      colour = actual,
      shape = prediction
    ),
    size = 3
  ) +
  coord_cartesian(
    xlim = c(-2.2,2.8),
    ylim = c(-3,3.5)
    ) +
  xlab(&ldquo;X&rdquo;) +
  ylab(&ldquo;Y&rdquo;)
{% endhighlight %}</p>

<p><img src="/figures/2016-05-08-decision-boundary-1.svg" alt="plot of chunk 2016-05-08-decision-boundary" /></p>

<p>I look forward to delving deeper into <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning</a>.</p>

<p>{% highlight r %}
sessionInfo()
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="r-version-3-3-1-2016-06-21">R version 3.3.1 (2016-06-21)</h2>

<h2 id="platform-x86-64-pc-linux-gnu-64-bit">Platform: x86_64-pc-linux-gnu (64-bit)</h2>

<h2 id="running-under-ubuntu-14-04-4-lts">Running under: Ubuntu 14.04.4 LTS</h2>

<h2 id="locale">locale:</h2>

<h2 id="1-lc-ctype-en-gb-utf-8-lc-numeric-c"><a href="1" title="From this equation onwards, I drop Hastie et al's convention of denoting matrices with $\hat{}$.
">1</a> LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C</h2>

<h2 id="3-lc-time-en-gb-utf-8-lc-collate-en-gb-utf-8"><a href="3" title="https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf
">3</a> LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8</h2>

<h2 id="5-lc-monetary-en-gb-utf-8-lc-messages-en-gb-utf-8">[5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8</h2>

<h2 id="7-lc-paper-en-gb-utf-8-lc-name-c">[7] LC_PAPER=en_GB.UTF-8       LC_NAME=C</h2>

<h2 id="9-lc-address-c-lc-telephone-c">[9] LC_ADDRESS=C               LC_TELEPHONE=C</h2>

<h2 id="11-lc-measurement-en-gb-utf-8-lc-identification-c">[11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C</h2>

<h2 id="attached-base-packages">attached base packages:</h2>

<h2 id="1-stats-graphics-grdevices-utils-datasets-base"><a href="1" title="From this equation onwards, I drop Hastie et al's convention of denoting matrices with $\hat{}$.
">1</a> stats     graphics  grDevices utils     datasets  base</h2>

<h2 id="other-attached-packages">other attached packages:</h2>

<h2 id="1-tibble-1-0-purrr-0-2-1-ggplot2-2-1-0-broom-0-4-0"><a href="1" title="From this equation onwards, I drop Hastie et al's convention of denoting matrices with $\hat{}$.
">1</a> tibble_1.0     purrr_0.2.1    ggplot2_2.1.0  broom_0.4.0</h2>

<h2 id="5-magrittr-1-5-dplyr-0-4-3-testthat-1-0-2-knitr-1-13-1">[5] magrittr_1.5   dplyr_0.4.3    testthat_1.0.2 knitr_1.13.1</h2>

<h2 id="loaded-via-a-namespace-and-not-attached">loaded via a namespace (and not attached):</h2>

<h2 id="1-rcpp-0-12-5-mnormt-1-5-4-munsell-0-4-3-colorspace-1-2-6"><a href="1" title="From this equation onwards, I drop Hastie et al's convention of denoting matrices with $\hat{}$.
">1</a> Rcpp_0.12.5      mnormt_1.5-4     munsell_0.4.3    colorspace_1.2-6</h2>

<h2 id="5-lattice-0-20-33-r6-2-1-2-stringr-1-0-0-plyr-1-8-4">[5] lattice_0.20-33  R6_2.1.2         stringr_1.0.0    plyr_1.8.4</h2>

<h2 id="9-tools-3-3-1-parallel-3-3-1-grid-3-3-1-nlme-3-1-128">[9] tools_3.3.1      parallel_3.3.1   grid_3.3.1       nlme_3.1-128</h2>

<h2 id="13-gtable-0-2-0-psych-1-6-4-dbi-0-4-1-digest-0-6-9">[13] gtable_0.2.0     psych_1.6.4      DBI_0.4-1        digest_0.6.9</h2>

<h2 id="17-lazyeval-0-2-0-assertthat-0-1-crayon-1-3-2-reshape2-1-4-1">[17] lazyeval_0.2.0   assertthat_0.1   crayon_1.3.2     reshape2_1.4.1</h2>

<h2 id="21-formatr-1-4-tidyr-0-5-1-evaluate-0-9-labeling-0-3">[21] formatR_1.4      tidyr_0.5.1      evaluate_0.9     labeling_0.3</h2>

<h2 id="25-stringi-1-1-1-methods-3-3-1-scales-0-4-0">[25] stringi_1.1.1    methods_3.3.1    scales_0.4.0</h2>

<p>{% endhighlight %}</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">From this equation onwards, I drop Hastie et al&rsquo;s convention of denoting matrices with $\hat{}$.
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
<li id="fn:2">One of the confusing parts of this notation is that we don&rsquo;t actually want to transpose $X\in\mathbb{R}^{10 \times 2}$ into $X^{T}\in\mathbb{R}^{2 \times 10}$, as $X^T$ will not be conformable with $\beta\in\mathbb{R}^{2 \times 1}$. Instead, we want an inner product which is $X \cdot \beta$ or each row of $X$ multiplied by each column of $\beta$; in R this is <code>X %*% beta</code>, <strong>not</strong> <code>t(X) %*% beta</code>.
 <a class="footnote-return" href="#fnref:2"><sup>[return]</sup></a></li>
<li id="fn:3"><a href="https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf">https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf</a>
 <a class="footnote-return" href="#fnref:3"><sup>[return]</sup></a></li>
<li id="fn:4"><a href="https://en.wikipedia.org/wiki/QR_decomposition">https://en.wikipedia.org/wiki/QR_decomposition</a>
 <a class="footnote-return" href="#fnref:4"><sup>[return]</sup></a></li>
</ol>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

