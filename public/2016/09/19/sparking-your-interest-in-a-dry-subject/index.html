<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.35" />


<title>Sparking your interest in a dry subject - A Hugo website</title>
<meta property="og:title" content="Sparking your interest in a dry subject - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">8 min read</span>
    

    <h1 class="article-title">Sparking your interest in a dry subject</h1>

    
    <span class="article-date">2016/09/19</span>
    

    <div class="article-content">
      

<p>I recently attended the conference Effective Applications of the R language in London. One of the many excellent speakers described how one can use <a href="https://www.r-bloggers.com/spark-2-0-more-performance-more-statistical-models/">Spark</a> to apply some simple Machine Learning to larger data sets and then extend the range of potential models by simply adding <a href="http://koaning.io/sparling-water-for-sparkr.html">water</a>.</p>

<p>We explore some of the main features and how to get started in this blog. Spark is a general purpose cluster computing system.</p>

<h2 id="installation">Installation</h2>

<p>Follow the guidance on <a href="https://github.com/rstudio/sparklyr">Github</a>.</p>

<h2 id="connecting-to-spark">Connecting to Spark</h2>

<p>Now we form a local Spark connection.</p>

<p>{% highlight r %}
library(sparklyr)
sc &lt;- spark_connect(master = &ldquo;local&rdquo;)  #  The Spark connection
{% endhighlight %}</p>

<h2 id="hadoop">Hadoop</h2>

<p>As I&rsquo;m running on Windows I get an error, I need to get an embedded copy of Hadoop winutils.exe from <a href="embedded copy of Hadoop winutils.exe">here</a>.</p>

<h2 id="java">Java</h2>

<p>I get another erorr, I need <a href="https://www.java.com/en/">Java</a> also! Success.</p>

<h2 id="reading-data">Reading data</h2>

<p>Typically one reads data within the Spark cluster using the <code>spark_read</code> family of functions. For convenience and reproducibility we use a small local data set also avaliable online at the <a href="https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength">UCI Machine Learning Repository</a>. Typically we might want to read from a remote SQL data table on a server.</p>

<p>We are interested in predicting the strength of concrete, a critical component of civil infrastructure, based on the non-linear relationship between it&rsquo;s ingredients and age. We read in the data and normalise all the quantitative variables.</p>

<p>{% highlight r %}
normalise &lt;- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}  #  custom function to normalise, OK as there are no NA</p>

<p>library(tidyverse)</p>

<p>concrete &lt;- read.csv(&ldquo;data/2016-09-19-concrete.csv&rdquo;, header = TRUE)</p>

<p>concrete_norm &lt;- concrete %&gt;%
  lapply(normalise) %&gt;%
  as.data.frame()</p>

<p>concrete_tbl &lt;- copy_to(sc, concrete_norm, &ldquo;concrete&rdquo;, overwrite = TRUE)
glimpse(concrete_tbl)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="observations-na">Observations: NA</h2>

<h2 id="variables-9">Variables: 9</h2>

<h2 id="cement-dbl-1-00000000000-1-00000000000-0-52625570776-0-52">$ cement       <dbl> 1.00000000000, 1.00000000000, 0.52625570776, 0.52&hellip;</h2>

<h2 id="slag-dbl-0-0000000000-0-0000000000-0-3964941569-0-39649">$ slag         <dbl> 0.0000000000, 0.0000000000, 0.3964941569, 0.39649&hellip;</h2>

<h2 id="ash-dbl-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0">$ ash          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0&hellip;</h2>

<h2 id="water-dbl-0-3210862620-0-3210862620-0-8482428115-0-84824">$ water        <dbl> 0.3210862620, 0.3210862620, 0.8482428115, 0.84824&hellip;</h2>

<h2 id="superplastic-dbl-0-07763975155-0-07763975155-0-00000000000-0-00">$ superplastic <dbl> 0.07763975155, 0.07763975155, 0.00000000000, 0.00&hellip;</h2>

<h2 id="coarseagg-dbl-0-6947674419-0-7383720930-0-3808139535-0-38081">$ coarseagg    <dbl> 0.6947674419, 0.7383720930, 0.3808139535, 0.38081&hellip;</h2>

<h2 id="fineagg-dbl-0-2057200201-0-2057200201-0-0000000000-0-00000">$ fineagg      <dbl> 0.2057200201, 0.2057200201, 0.0000000000, 0.00000&hellip;</h2>

<h2 id="age-dbl-0-074175824176-0-074175824176-0-739010989011-1">$ age          <dbl> 0.074175824176, 0.074175824176, 0.739010989011, 1&hellip;</h2>

<h2 id="strength-dbl-0-96748473901-0-74199576430-0-47265479008-0-48">$ strength     <dbl> 0.96748473901, 0.74199576430, 0.47265479008, 0.48&hellip;</h2>

<p>{% endhighlight %}</p>

<h2 id="machine-learning">Machine Learning</h2>

<p>You can orchestrate machine learning algorithms in a Spark cluster via the machine learning functions within &lsquo;sparklyr&rsquo;. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows. We demonstrate a few of these here.</p>

<p>We start by:</p>

<ol>
<li>Partition the data into separate training and test data sets,</li>
<li>Fit a model to our training data set,</li>
<li>Evaluate our predictive performance on our test dataset.</li>
</ol>

<p>{% highlight r %}</p>

<h1 id="transform-our-data-set-and-then-partition-into-training-test">transform our data set, and then partition into &lsquo;training&rsquo;, &lsquo;test&rsquo;</h1>

<p>partitions &lt;- concrete_tbl %&gt;%
  sdf_partition(training = 0.75, test = 0.25, seed = 1337)</p>

<h1 id="fit-a-linear-mdoel-to-the-training-dataset">fit a linear mdoel to the training dataset</h1>

<p>fit &lt;- partitions$training %&gt;%
  ml_linear_regression(strength ~.)
print(fit)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="call-ml-linear-regression-strength">Call: ml_linear_regression(., strength ~ .)</h2>

<h2 id="coefficients">Coefficients:</h2>

<h2 id="intercept-cement-slag-ash-water">(Intercept)         cement           slag            ash          water</h2>

<h2 id="0-01902770740-0-62427687298-0-42447567739-0-20453735084-0-28004107400">0.01902770740  0.62427687298  0.42447567739  0.20453735084 -0.28004107400</h2>

<h2 id="superplastic-coarseagg-fineagg-age">superplastic      coarseagg        fineagg            age</h2>

<h2 id="0-08553133565-0-03995171111-0-06916150314-0-53664290261">0.08553133565  0.03995171111  0.06916150314  0.53664290261</h2>

<p>{% endhighlight %}</p>

<p>For linear regression models produced by Spark, we can use <code>summary()</code> to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.</p>

<p>{% highlight r %}
summary(fit)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="call-ml-linear-regression-strength-1">Call: ml_linear_regression(., strength ~ .)</h2>

<h2 id="deviance-residuals">Deviance Residuals::</h2>

<h2 id="min-1q-median-3q-max">Min          1Q      Median          3Q         Max</h2>

<h2 id="0-37387125-0-07918751-0-01039496-0-08263571-0-43699505">-0.37387125 -0.07918751  0.01039496  0.08263571  0.43699505</h2>

<h2 id="coefficients-1">Coefficients:</h2>

<h2 id="estimate-std-error-t-value-pr-t">Estimate   Std. Error  t value               Pr(&gt;|t|)</h2>

<h2 id="intercept-0-019027707-0-120060491-0-15848-0-87411645">(Intercept)   0.019027707  0.120060491  0.15848             0.87411645</h2>

<h2 id="cement-0-624276873-0-054064454-11-54690-0-000000000000000222">cement        0.624276873  0.054064454 11.54690 &lt; 0.000000000000000222 ***</h2>

<h2 id="slag-0-424475677-0-052993333-8-00998-0-0000000000000042188">slag          0.424475677  0.052993333  8.00998  0.0000000000000042188 ***</h2>

<h2 id="ash-0-204537351-0-036911306-5-54132-0-0000000411567488978">ash           0.204537351  0.036911306  5.54132  0.0000000411567488978 ***</h2>

<h2 id="water-0-280041074-0-074010191-3-78382-0-00016635">water        -0.280041074  0.074010191 -3.78382             0.00016635 ***</h2>

<h2 id="superplastic-0-085531336-0-046414564-1-84277-0-06574445">superplastic  0.085531336  0.046414564  1.84277             0.06574445 .</h2>

<h2 id="coarseagg-0-039951711-0-046286492-0-86314-0-38832767">coarseagg     0.039951711  0.046286492  0.86314             0.38832767</h2>

<h2 id="fineagg-0-069161503-0-061500842-1-12456-0-26112290">fineagg       0.069161503  0.061500842  1.12456             0.26112290</h2>

<h2 id="age-0-536642903-0-030283336-17-72073-0-000000000000000222">age           0.536642903  0.030283336 17.72073 &lt; 0.000000000000000222 ***</h2>

<h2 id="toc_40">&mdash;</h2>

<h2 id="signif-codes-0-0-001-0-01-0-05-0-1-1">Signif. codes:  0 &lsquo;*<strong>&rsquo; 0.001 &lsquo;</strong>&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1</h2>

<h2 id="r-squared-0-5901">R-Squared: 0.5901</h2>

<h2 id="root-mean-squared-error-0-1306">Root Mean Squared Error: 0.1306</h2>

<p>{% endhighlight %}</p>

<p>The summary suggest our model is a poor-fit. We need to account for the non-linear relationships in the data, something which the linear model fails at! Let&rsquo;s test our model against data we havn&rsquo;t seen to have an indictation of its error.</p>

<p>{% highlight r %}</p>

<h1 id="compute-predicted-values-on-our-test-dataset">compute predicted values on our test dataset</h1>

<p>predicted &lt;- predict(fit, newdata = partitions$test)</p>

<h1 id="extract-the-true-strength-values-from-our-test-dataset">extract the true &lsquo;strength&rsquo; values from our test dataset</h1>

<p>actual &lt;- partitions$test %&gt;%
  select(strength) %&gt;%
  collect() %&gt;%
  <code>[[</code>(&ldquo;strength&rdquo;)</p>

<h1 id="produce-a-data-frame-housing-our-predicted-actual-strength-values">produce a data.frame housing our predicted + actual &lsquo;strength&rsquo; values</h1>

<p>data &lt;- data.frame(
  predicted = predicted,
  actual    = actual
)</p>

<h1 id="plot-predicted-vs-actual-values">plot predicted vs. actual values</h1>

<p>ggplot(data, aes(x = actual, y = predicted)) +
  geom_abline(lty = &ldquo;dashed&rdquo;, col = &ldquo;red&rdquo;) +
  geom_point() +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_fixed(ratio = 1) +
  labs(
    x = &ldquo;Actual Strength&rdquo;,
    y = &ldquo;Predicted Strength&rdquo;,
    title = &ldquo;Predicted vs. Actual Concrete Strength&rdquo;
  )
{% endhighlight %}</p>

<p><img src="/figures2016-09-19_lm-1.svg" alt="plot of chunk 2016-09-19_lm" /></p>

<p>Not bad, but then again not so good. More importantly our diagnostic plots reveal heteroschedasticity and other problems which suggest a linear model is inappropriate for this data.</p>

<p>{% highlight r %}</p>

<h1 id="function-that-returns-root-mean-squared-error">Function that returns Root Mean Squared Error</h1>

<p>rmse &lt;- function(error)
{
    sqrt(mean(error^2))
}</p>

<h1 id="function-that-returns-mean-absolute-error">Function that returns Mean Absolute Error</h1>

<p>mae &lt;- function(error)
{
    mean(abs(error))
}</p>

<h1 id="calculate-error">Calculate error</h1>

<p>error &lt;- actual - predicted</p>

<h1 id="example-of-invocation-of-functions">Example of invocation of functions</h1>

<p>rmse(error)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-0-1245939277">[1] 0.1245939277</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
mae(error)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-0-09907686709">[1] 0.09907686709</h2>

<p>{% endhighlight %}</p>

<p>This is a building critical ingredient, we have a duty of care to do better. We opt for a ML method that can handle non-linear relationships, a neural network approach.</p>

<h2 id="neural-network">Neural Network</h2>

<p>We follow the same workflow using a Multilayer Perceptron. We fit the model.</p>

<p>{% highlight r %}</p>

<h1 id="fit-a-non-linear-mdoel-to-the-training-dataset">fit a non-linear mdoel to the training dataset</h1>

<p>fit_nn &lt;- partitions$training %&gt;%
  ml_multilayer_perceptron(strength~. , layers =  c(8, 30, 20), seed = 255)
{% endhighlight %}
Let&rsquo;s compare our predictions with the actual. Predict doesn&rsquo;t recognise the <code>fit_nn</code> object, and gives us predictions of zero. As this is relatively new I failed to find any supporting documentation to fix this. Instead I used the <code>nnet</code> package to fit then <code>compute</code> the predicted strength using a neural network, sadly not using Spark.</p>

<p>{% highlight r %}
library(neuralnet)</p>

<h1 id="compute-predicted-values-on-our-test-dataset-1"># compute predicted values on our test dataset</h1>

<h1 id="predicted-predict-fit-nn-newdata-partitions-test-fails">predicted &lt;- predict(fit_nn, newdata = partitions$test)  #  Fails!</h1>

<p>#PARTITION DATA
concrete_train &lt;- concrete_norm[1:773, ] #  75%
concrete_test &lt;- concrete_norm[774:1030, ]#  25%, it&rsquo;s easy to overfit a neural network</p>

<p>#MODEL 2, more hidden nodes
concrete_model2 &lt;- neuralnet(strength ~ cement + slag + ash + water +
                               superplastic + coarseagg +
                               fineagg + age,
                             data = concrete_train, hidden = 5)
plot(concrete_model2)</p>

<p>model_results &lt;- compute(concrete_model2,concrete_test[1:8])  # columns 1 to 8, 9 is the strength
predicted_strength &lt;- model_results$net.result</p>

<p>cor(predicted_strength, concrete_test$strength)[ , 1]  # can vary depending on random seed
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-0-7120350563">[1] 0.7120350563</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
plot(predicted_strength, concrete_test$strength)  # line em up, aidvisualisation
abline(a = 0, b = 1)
{% endhighlight %}</p>

<p>Let&rsquo;s quantify the error of the model and compare to the linear model earlier.</p>

<p>{% highlight r %}</p>

<h1 id="calculate-error-1">Calculate error</h1>

<p>error &lt;- concrete_test$strength - predicted_strength</p>

<h1 id="example-of-invocation-of-functions-1">Example of invocation of functions</h1>

<p>rmse(error)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-0-1287046052">[1] 0.1287046052</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
mae(error)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-0-09727616659">[1] 0.09727616659</h2>

<p>{% endhighlight %}
The error has been reduced! Seems like a non-linear approach was superior for this type of problem. Let me know in the comments how I can predict using the <code>ml_multilayer_perceptron()</code> function in Spark.</p>

<h2 id="principal-component-analysis">Principal Component Analysis</h2>

<p>There&rsquo;s lots of standard <a href="http://spark.rstudio.com/mllib.html">ML stuff</a> you can apply to your data.</p>

<p>Use Spark&rsquo;s Principal Components Analysis (PCA) to perform dimensionality reduction. PCA is a <a href="https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html">statistical method</a> to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible. Not particularly useful here but might be useful for those Kaggle competitions.</p>

<p>{% highlight r %}
pca_model &lt;- tbl(sc, &ldquo;concrete&rdquo;) %&gt;%
  select(-strength) %&gt;%
  ml_pca()
print(pca_model)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="explained-variance">Explained variance:</h2>

<h2 id="not-available-in-this-version-of-spark">[not available in this version of Spark]</h2>

<h2 id="rotation">Rotation:</h2>

<h2 id="pc1-pc2-pc3-pc4">PC1            PC2            PC3            PC4</h2>

<h2 id="cement-0-29441188077-0-56727866003-0-41402394827-0-35592709219">cement       -0.29441188077  0.56727866003 -0.41402394827  0.35592709219</h2>

<h2 id="slag-0-25817851997-0-73049769143-0-09807456987-0-03945308009">slag         -0.25817851997 -0.73049769143 -0.09807456987 -0.03945308009</h2>

<h2 id="ash-0-83948585853-0-06617190283-0-09619819590-0-38363862495">ash           0.83948585853 -0.06617190283  0.09619819590  0.38363862495</h2>

<h2 id="water-0-20074746639-0-13313840133-0-29975480560-0-37899852407">water        -0.20074746639 -0.13313840133  0.29975480560  0.37899852407</h2>

<h2 id="superplastic-0-23602883744-0-03284027384-0-49184647576-0-01313672992">superplastic  0.23602883744 -0.03284027384 -0.49184647576 -0.01313672992</h2>

<h2 id="coarseagg-0-03020285667-0-33836766789-0-60538891266-0-38459602718">coarseagg     0.03020285667  0.33836766789  0.60538891266 -0.38459602718</h2>

<h2 id="fineagg-0-17805072250-0-06232116938-0-30465072262-0-61340964581">fineagg       0.17805072250  0.06232116938 -0.30465072262 -0.61340964581</h2>

<h2 id="age-0-11534972874-0-05484888012-0-13652017635-0-23787141980">age          -0.11534972874  0.05484888012  0.13652017635  0.23787141980</h2>

<h2 id="pc5-pc6-pc7-pc8">PC5            PC6            PC7            PC8</h2>

<h2 id="cement-0-21502882158-0-07652048639-0-29249504460-0-39467773130">cement       -0.21502882158  0.07652048639 -0.29249504460 -0.39467773130</h2>

<h2 id="slag-0-37381516136-0-10484674144-0-30139327675-0-38337095161">slag         -0.37381516136 -0.10484674144 -0.30139327675 -0.38337095161</h2>

<h2 id="ash-0-05533257884-0-01869801102-0-26647180272-0-24501745575">ash          -0.05533257884  0.01869801102 -0.26647180272 -0.24501745575</h2>

<h2 id="water-0-37579073697-0-22258795780-0-48270234625-0-53358800969">water         0.37579073697  0.22258795780  0.48270234625 -0.53358800969</h2>

<h2 id="superplastic-0-35449726457-0-27612080966-0-69970708443-0-09810891299">superplastic -0.35449726457 -0.27612080966  0.69970708443 -0.09810891299</h2>

<h2 id="coarseagg-0-47740668504-0-12968952955-0-08267907823-0-34440226786">coarseagg    -0.47740668504 -0.12968952955  0.08267907823 -0.34440226786</h2>

<h2 id="fineagg-0-50199453465-0-06691264853-0-12126149143-0-47344523553">fineagg       0.50199453465 -0.06691264853 -0.12126149143 -0.47344523553</h2>

<h2 id="age-0-25329921522-0-91417579451-0-09203141642-0-01085413355">age           0.25329921522 -0.91417579451 -0.09203141642 -0.01085413355</h2>

<p>{% endhighlight %}</p>

<h2 id="conclusion">Conclusion</h2>

<p>This blog described how to get Spark on your machine and use it to conduct some basic ML. It should be useful when dealing with large data sets or interacting with remote data tables on SQL servers. The sustained improvements in all things R continues to inspire and amaze.</p>

<p>{% highlight r %}
sessionInfo()
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="r-version-3-3-1-2016-06-21">R version 3.3.1 (2016-06-21)</h2>

<h2 id="platform-x86-64-w64-mingw32-x64-64-bit">Platform: x86_64-w64-mingw32/x64 (64-bit)</h2>

<h2 id="running-under-windows-8-x64-build-9200">Running under: Windows &gt;= 8 x64 (build 9200)</h2>

<h2 id="locale">locale:</h2>

<h2 id="1-lc-collate-english-united-kingdom-1252">[1] LC_COLLATE=English_United Kingdom.1252</h2>

<h2 id="2-lc-ctype-english-united-kingdom-1252">[2] LC_CTYPE=English_United Kingdom.1252</h2>

<h2 id="3-lc-monetary-english-united-kingdom-1252">[3] LC_MONETARY=English_United Kingdom.1252</h2>

<h2 id="4-lc-numeric-c">[4] LC_NUMERIC=C</h2>

<h2 id="5-lc-time-english-united-kingdom-1252">[5] LC_TIME=English_United Kingdom.1252</h2>

<h2 id="attached-base-packages">attached base packages:</h2>

<h2 id="1-stats-graphics-grdevices-utils-datasets-methods-base">[1] stats     graphics  grDevices utils     datasets  methods   base</h2>

<h2 id="other-attached-packages">other attached packages:</h2>

<h2 id="1-neuralnet-1-33-dplyr-0-5-0-purrr-0-2-2-readr-1-0-0">[1] neuralnet_1.33  dplyr_0.5.0     purrr_0.2.2     readr_1.0.0</h2>

<h2 id="5-tidyr-0-6-0-tibble-1-2-ggplot2-2-1-0-tidyverse-1-0-0">[5] tidyr_0.6.0     tibble_1.2      ggplot2_2.1.0   tidyverse_1.0.0</h2>

<h2 id="9-sparklyr-0-3-14">[9] sparklyr_0.3.14</h2>

<h2 id="loaded-via-a-namespace-and-not-attached">loaded via a namespace (and not attached):</h2>

<h2 id="1-rcpp-0-12-7-knitr-1-14-magrittr-1-5-rappdirs-0-3-1">[1] Rcpp_0.12.7      knitr_1.14       magrittr_1.5     rappdirs_0.3.1</h2>

<h2 id="5-munsell-0-4-3-colorspace-1-2-6-r6-2-1-3-stringr-1-1-0">[5] munsell_0.4.3    colorspace_1.2-6 R6_2.1.3         stringr_1.1.0</h2>

<h2 id="9-plyr-1-8-4-tools-3-3-1-parallel-3-3-1-grid-3-3-1">[9] plyr_1.8.4       tools_3.3.1      parallel_3.3.1   grid_3.3.1</h2>

<h2 id="13-rmd2md-0-1-0-gtable-0-2-0-config-0-2-dbi-0-5-1">[13] rmd2md_0.1.0     gtable_0.2.0     config_0.2       DBI_0.5-1</h2>

<h2 id="17-withr-1-0-2-lazyeval-0-2-0-yaml-2-1-13-assertthat-0-1">[17] withr_1.0.2      lazyeval_0.2.0   yaml_2.1.13      assertthat_0.1</h2>

<h2 id="21-rprojroot-1-0-2-digest-0-6-10-formatr-1-4-evaluate-0-9">[21] rprojroot_1.0-2  digest_0.6.10    formatR_1.4      evaluate_0.9</h2>

<h2 id="25-labeling-0-3-stringi-1-1-1-scales-0-4-0">[25] labeling_0.3     stringi_1.1.1    scales_0.4.0</h2>

<p>{% endhighlight %}</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

