<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.35" />


<title>Newyearal Networks - A Hugo website</title>
<meta property="og:title" content="Newyearal Networks - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">13 min read</span>
    

    <h1 class="article-title">Newyearal Networks</h1>

    
    <span class="article-date">2017/01/03</span>
    

    <div class="article-content">
      

<p>One of my New Year resolutions is to get to grips with deep learning.
I thought a good place to start would be a refresher into &lsquo;shallow&rsquo; neural networks, and that is what this post and the one that follows it will be about.
I&rsquo;ll go through the maths behind a quick dirty neural network, and implement it in R.</p>

<p>I&rsquo;m using as my sources the tome <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">Elements of Statistical Learning</a>, Andrew Ng&rsquo;s excellent <a href="https://www.coursera.org/learn/machine-learning">machine learning</a> course on coursera, and a short course I have been doing on Udemy: <a href="https://www.udemy.com/data-science-deep-learning-in-python/learn/v4/">Deep Learning in Python</a>.
I can recommend them all.</p>

<h2 id="an-example">An example</h2>

<p>I&rsquo;m going to deal with a very simple neural network here: a single hidden layer network trained with back-propagation, and starting out with a sigmoid activation function.</p>

<p>I&rsquo;ll use just two input nodes $x_1$ and $x_2$, set the hidden layer to have just four nodes ($z_1$, $z_2$, $z_3$, and $z_4$), with a single output node $y$.
So, my obligatory network diagram should look like:</p>

<p><img src='http://www.machinegurning.com/figures/2017-01-03-neural_network.svg' alt='Neural Network' style='display: block; margin: auto; padding: 10px 10px;'></p>

<p>Note that I have included additional &lsquo;bias&rsquo; nodes to $X$ and $Z$ (the top ones: $x_0$ and $z_0$), which are always equal to one.
I&rsquo;ll explain what this is for as we go along.</p>

<h2 id="the-maths">The maths</h2>

<p>So how does it work?
Once we have set the architecture of the network, there are two processes that we can do with our network: training, and prediction.</p>

<p>For predicting, Hastie et al. give the following equations:</p>

<p>$$
\begin{array}{ll}<br />
Z<em>m = \sigma(\alpha</em>{0m} + \alpha_m^TX),:\text{for}:m={1, \ldots, M}
    &amp; <br />
T<em>k = \beta</em>{0k} + \beta_k^TZ,:\text{for}:k={1, \ldots, K}
    &amp; <br />
f_k(X) = g_k(T),:\text{for}:k={1, \ldots, K}
\end{array}
$$</p>

<p>So here, $Z$ is our hidden layer, which takes the input matrix $X$ and multiplies this with the matrix transpose of $\alpha$, which is our first matrix of weights.
Here, Hastie, et al. refer to the bias as $\alpha<em>{0m}$ (and $\beta</em>{0k}$).
Straight away this tells us that $\alpha$ must have the same number of rows as $X$, so that when transposed ($\alpha^T$) it will have the same number of columns ($M$) as there are rows (training examples) in our design matrix $X$.
Here $\sigma$ refers to the logistic function:</p>

<p>$$
\sigma(v) = \dfrac{1}{1+e^{-v}}
$$</p>

<p>The second equation is essentially the same except it does not apply the logistic function, instead passing onto the third equation where the function $g_k$ is applied instead.
Here $g_k$ is the softmax function:</p>

<p>$$
g_k(T) = \dfrac{e^{T<em>k}}{\sum^K</em>{l=1}e^{T_l}}
$$</p>

<p>To keep this example really simple I&rsquo;m going to use the sigmoid logistic function throughout, but I&rsquo;ll come back to talking about softmax in a later post.
Note that $K$ is the number of nodes in the vector $Z$, so again, we know that $\beta$ must have the same number of rows as $Z$ ($K$) in order that it has the correct number of columns when transposed and multiplied with $Z$.</p>

<p>Along with including the bias nodes in the vectors $X$ and $Z$, this simplification allows us to simplify the equations into:</p>

<p>$$
\begin{array}{ll}<br />
Z_m = \sigma(\alpha_m^TX),:\text{for}:m={1, \ldots, M} <br />
y = \sigma(\beta_k^TZ),:\text{for}:k={1, \ldots, K}
\end{array}
$$</p>

<p>If you&rsquo;ve done any of Andrew Ng&rsquo;s machine learning course, or read any of my earlier blogs about this, this should be starting to look awfully familiar - we are essentially using the vectorised form of logistic regression.</p>

<p>Written out in full, for the network architecture I outlined above, $Z$ becomes:</p>

<p>$$
\begin{array}{ll}<br />
z_0 = 1 <br />
z<em>1 = \sigma(\alpha</em>{0,0}x<em>0 + \alpha</em>{1,0}x<em>1 + \alpha</em>{2,0} x_2) <br />
z<em>2 = \sigma(\alpha</em>{0,1}x<em>0 + \alpha</em>{1,1}x<em>1 + \alpha</em>{2,1} x_2) <br />
z<em>3 = \sigma(\alpha</em>{0,2}x<em>0 + \alpha</em>{1,2}x<em>1 + \alpha</em>{2,2} x_2) <br />
z<em>4 = \sigma(\alpha</em>{0,3}x<em>0 + \alpha</em>{1,3}x<em>1 + \alpha</em>{2,3} x_2) <br />
\end{array}
$$</p>

<p>So essentially we need to do an element-wise multiplication of the input matrix $X$, and a weight corresponding to each element of the vector $Z$.
From this we know that their are $M\times K$ weights in $\alpha$, and since we already know that $\alpha$ must have the same number of rows as $X$, we know that it must have the same number of columns as there are nodes in $Z$ (not counting the bias node $Z_0$ which is added later).</p>

<p>For my example then, $\alpha$ must be of dimensions $M \times K$.
The same can be said of $\beta$, except that we know it must be of dimensions $K \times 1$, as there is just a single output node $y$.</p>

<p>In this first example, I will set the matrices of weights $\alpha$ and $\beta$ to be entirely composed of ones.
I set $X$ to be comprise just $[0,1]$, along with the bias node $x_0=1$, so that:</p>

<p>$$</p>

<p>\begin{array}{ll}</p>

<p>X=\begin{bmatrix}
1 <br />
0 <br />
1 <br />
\end{bmatrix}</p>

<p>&amp;</p>

<p>\alpha=\begin{bmatrix}
1 &amp; 1 &amp; 1 &amp; 1 <br />
1 &amp; 1 &amp; 1 &amp; 1 <br />
1 &amp; 1 &amp; 1 &amp; 1 <br />
\end{bmatrix}</p>

<p>&amp;</p>

<p>\beta=\begin{bmatrix}
1 \ 1 \ 1 \ 1 \ 1 <br />
\end{bmatrix}</p>

<p><br />
\end{array}
$$</p>

<p>And, to recap on the dimensions of each of these matrices:</p>

<p>$$
\begin{array}{ll}
X\in\mathbb{R}^{(2 + 1)\times1} &amp;
\alpha\in\mathbb{R}^{3\times4} &amp;
Z\in\mathbb{R}^{(4+1)\times1} &amp;
\beta\in\mathbb{R}^{5\times1} &amp;
y\in\mathbb{R}^1
\end{array}
$$</p>

<h3 id="implementing-this-in-r">Implementing this in R</h3>

<p>Implementing this in R is very simple.
Recall that I have chosen to use the logistic function throughout, so I do not need to define the softmax function at this point (unlike Hastie et al.).</p>

<p>{% highlight r %}</p>

<h1 id="define-the-sigmoid-function">Define the sigmoid function</h1>

<p>s &lt;- function(x) 1/(1+exp(-x))</p>

<h1 id="define-the-matrix-x-including-the-bias-node-x-0-note-that-r-is-smart-enough">Define the matrix X (including the bias node x_0). Note that R is smart enough</h1>

<h1 id="to-to-coerce-x-into-a-matrix-so-we-can-pass-it-a-row-vector-with-c-instead">to to coerce X into a matrix, so we can pass it a row vector with c(), instead</h1>

<h1 id="of-a-column-vector">of a column vector.</h1>

<p>X &lt;- c(1,0,1)
M &lt;- length(X)</p>

<h1 id="setting-k-here-sets-the-number-of-nodes-in-the-input-layer">Setting K here sets the number of nodes in the input layer</h1>

<p>K &lt;- 5</p>

<h1 id="set-up-weights-matrices-note-that-i-use-k-1-in-a-because-we-only-add-the">Set up weights matrices. Note that I use K - 1 in a because we only add the</h1>

<h1 id="bias-node-z-0-after-first-calculating-z">bias node z_0 after first calculating Z</h1>

<p>a &lt;- matrix(rep(1, M * (K - 1)), nrow = M)
b &lt;- matrix(rep(1, K), nrow = K)</p>

<h1 id="transpose-and-multiply-a-with-x-then-add-the-bias-node">Transpose and multiply a with X, then add the bias node.</h1>

<p>Z &lt;- s(t(a) %*% X)
Z &lt;- rbind(1, Z)</p>

<h1 id="finally-calculate-y">Finally calculate y</h1>

<p>y &lt;- s(t(b) %*% Z)</p>

<p>y
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1">[,1]</h2>

<h2 id="1-0-9892622">[1,] 0.9892622</h2>

<p>{% endhighlight %}</p>

<p>It would be good to tidy this up into a more generic function which we can test.
Such a function will need to take a number of inputs: an input vector (or matrix) $X$, and two sets of weights $\alpha$ and $\beta$.</p>

<p>{% highlight r %}
nn_predict &lt;- function(x, alpha, beta) {</p>

<p># Define the matrix extents</p>

<p>M = length(x)
  K = ncol(alpha) + 1</p>

<p># Define the sigmoid function</p>

<p>s &lt;- function(x) 1 / (1 + exp(-x))</p>

<p># Check whether there is more than one row in X, and if so use apply to run
  # over the whole matrix. Note here I use crossprod instead of %*% which is
  # slightly more performant.</p>

<p>Z &lt;- if (is.null(nrow(x))) s(crossprod(alpha, x)) else
    apply(x, 1, function(x) s(crossprod(alpha,x)))</p>

<p># Add the bias node to Z, and calculate beta&rsquo;Z</p>

<p>Z &lt;- rbind(1, Z)
  y &lt;- s(crossprod(beta,Z))</p>

<p># Transpose and convert to a dataframe to be matched against origin data</p>

<p>y &lt;- data.frame(pred = t(y))</p>

<p>return(y)</p>

<p>}
{% endhighlight %}</p>

<p>So using the same inputs as before&hellip;</p>

<p>{% highlight r %}
nn_predict(X, a, b)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="pred">pred</h2>

<h2 id="1-0-9892622-1">1 0.9892622</h2>

<p>{% endhighlight %}</p>

<p>Seems legit.
Our neural network prediction should also work under instances when our input data is a matrix of values, i.e. a dataset with more than one training example $X$.</p>

<p>I&rsquo;ll try first with just two identical rows:</p>

<p>{% highlight r %}
X &lt;- unname(rbind(X, X))</p>

<p>nn_predict(X, a, b)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="pred-1">pred</h2>

<h2 id="1-0-9892622-2">1 0.9892622</h2>

<h2 id="2-0-9892622">2 0.9892622</h2>

<p>{% endhighlight %}</p>

<p>So far so good.
Now what about with the <code>mtcars</code> dataset.
Note that on real data it is noral practice to normalise each variable to a mean of $0$ and a standard deviation of $1$; failing to do so can have a very negative effect on the training of the network.</p>

<p>{% highlight r %}</p>

<h1 id="define-the-normalise-function">Define the normalise function</h1>

<p>normalise &lt;- function(x) {</p>

<p>y &lt;- (x - mean(x))/sd(x)
  return(y)</p>

<p>}</p>

<h1 id="normalise-the-mtcars-data-add-bias-node-and-convert-to-matrix">Normalise the mtcars data, add bias node, and convert to matrix</h1>

<p>X &lt;- apply(mtcars, 2, normalise)
X &lt;- as.matrix(cbind(1, X))</p>

<p>M &lt;- ncol(X)
K &lt;- 5</p>

<h1 id="when-sacling-the-input-variables-it-is-usual-to-use-random-weights-with-a">When sacling the input variables it is usual to use random weights with a</h1>

<h1 id="uniform-distribution-between-0-7-and-0-7-following-hastie-et-al">uniform distribution between -0.7 and 0.7, following Hastie, et al.</h1>

<p>a &lt;- matrix(runif(M *(K - 1), min = -0.7, max = 0.7), nrow = M)
b &lt;- matrix(runif(K, min = -0.7, max = 0.7), nrow = K)</p>

<p>nn_predict(X, a, b)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="pred-2">pred</h2>

<h2 id="mazda-rx4-0-3548774">Mazda RX4           0.3548774</h2>

<h2 id="mazda-rx4-wag-0-3511640">Mazda RX4 Wag       0.3511640</h2>

<h2 id="datsun-710-0-2973615">Datsun 710          0.2973615</h2>

<h2 id="hornet-4-drive-0-2728596">Hornet 4 Drive      0.2728596</h2>

<h2 id="hornet-sportabout-0-3278089">Hornet Sportabout   0.3278089</h2>

<h2 id="valiant-0-2655280">Valiant             0.2655280</h2>

<h2 id="duster-360-0-3385054">Duster 360          0.3385054</h2>

<h2 id="merc-240d-0-2843532">Merc 240D           0.2843532</h2>

<h2 id="merc-230-0-2629109">Merc 230            0.2629109</h2>

<h2 id="merc-280-0-2808949">Merc 280            0.2808949</h2>

<h2 id="merc-280c-0-2694809">Merc 280C           0.2694809</h2>

<h2 id="merc-450se-0-3238025">Merc 450SE          0.3238025</h2>

<h2 id="merc-450sl-0-3196923">Merc 450SL          0.3196923</h2>

<h2 id="merc-450slc-0-3065954">Merc 450SLC         0.3065954</h2>

<h2 id="cadillac-fleetwood-0-3094050">Cadillac Fleetwood  0.3094050</h2>

<h2 id="lincoln-continental-0-3151334">Lincoln Continental 0.3151334</h2>

<h2 id="chrysler-imperial-0-3437211">Chrysler Imperial   0.3437211</h2>

<h2 id="fiat-128-0-3283872">Fiat 128            0.3283872</h2>

<h2 id="honda-civic-0-3270231">Honda Civic         0.3270231</h2>

<h2 id="toyota-corolla-0-3291828">Toyota Corolla      0.3291828</h2>

<h2 id="toyota-corona-0-2633779">Toyota Corona       0.2633779</h2>

<h2 id="dodge-challenger-0-3175960">Dodge Challenger    0.3175960</h2>

<h2 id="amc-javelin-0-3048592">AMC Javelin         0.3048592</h2>

<h2 id="camaro-z28-0-3400135">Camaro Z28          0.3400135</h2>

<h2 id="pontiac-firebird-0-3373499">Pontiac Firebird    0.3373499</h2>

<h2 id="fiat-x1-9-0-3028328">Fiat X1-9           0.3028328</h2>

<h2 id="porsche-914-2-0-3790375">Porsche 914-2       0.3790375</h2>

<h2 id="lotus-europa-0-3698069">Lotus Europa        0.3698069</h2>

<h2 id="ford-pantera-l-0-4161256">Ford Pantera L      0.4161256</h2>

<h2 id="ferrari-dino-0-3995544">Ferrari Dino        0.3995544</h2>

<h2 id="maserati-bora-0-4261293">Maserati Bora       0.4261293</h2>

<h2 id="volvo-142e-0-2963969">Volvo 142E          0.2963969</h2>

<p>{% endhighlight %}</p>

<p>Great, so the &lsquo;feed-forward&rsquo; part works - we are able to predict using the neural network.
Of course, without training our network first, these predictions are meaningless.
My next post will deal with training the neural network using &lsquo;back-propagation&rsquo;, so that we can use the <code>nn_predict</code> function for classification.
To round out this post, I&rsquo;ll recap on why we need those pesky bias nodes.</p>

<h2 id="so-why-a-bias-node-again">So why a bias node again??</h2>

<p>The easiest way to understand this is to look back at a simple univariate linear regression.
I&rsquo;ve <a href="http://www.machinegurning.com/rstats/linear_regression/">blogged</a> about this in some detail in the past, but I will recap it quickly here.</p>

<p>Recall the linear regression model for a single case:</p>

<p>$$
y = a + bx + \epsilon
$$</p>

<p>For weights $a = 10$ and $b=20$, and $x=1$ (and ignoring error $\epsilon$), we would expect $y=30$:</p>

<p>$$
\begin{array}{rl}
y = &amp;a + bx <br />
y = &amp;10 + 20x <br />
y = &amp;10 + (20 \times1) <br />
y = &amp;30 <br />
\end{array}
$$</p>

<p>This can be restated in matrix terms as $y=\alpha^{T}x$ where $\alpha$ is a vector of weights, but if we leave $x$ unaltered, we get the following:</p>

<p>$$
\begin{array}{rl}
h(\alpha) = &amp; \alpha^Tx <br />
= &amp; \begin{bmatrix}10\20\end{bmatrix}^{T}\times1 <br />
= &amp; \begin{bmatrix}10\times1&amp;20\times1\end{bmatrix} <br />
= &amp; \begin{bmatrix}10&amp;20\end{bmatrix} <br />
\end{array}
$$</p>

<p>So that&rsquo;s no good as a vector multiplied by a scalar is still a vector.
If we actually want to get $30$, we need to add an additional column to $x_1$, converting it to a matrix $X$, containing columns $x_0$, and $x_1$.
Note that when $\alpha^{T}X$ works for a single column vector, for the matrix $X$ we simply use $X\alpha$.</p>

<p>$$
\begin{array}{rl}
h(\alpha) = &amp; \alpha^TX <br />
= &amp; \begin{bmatrix}1&amp;1\end{bmatrix}\begin{bmatrix}10\20\end{bmatrix} <br />
= &amp; (10\times1) + (20\times1) <br />
= &amp; 30 <br />
\end{array}
$$</p>

<p>In a more realistic situation where we have multiple training examples ($M=5$), and we have a single column in $X$ (and our bias node), it is still common to write $\alpha^{T}X$, but actually this is not quite what we mean.
We can&rsquo;t multiply $\alpha^{T}\in\mathbb{R}^{1\times2}$ by $X\in\mathbb{R}^{5,2}$, as the the number of rows of $\alpha^{T}$ does not equal the number of columns of $X$. We really mean $\alpha^{T}X$ applied row-wise, and $\alpha^{T}\in\mathbb{R}^{2\times1}$ by $X\in\mathbb{R}^{1,2}$ does work.
We can achieve the same effect with $X\alpha$.</p>

<p>{% highlight r %}</p>

<h1 id="when-m-1">When m = 1</h1>

<p>a &lt;- matrix(c(10,20), nrow = 2, ncol = 1)
X &lt;- matrix(rep(1,2), nrow = 1, ncol = 2)
t(a) %*% X[1,]
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-1">[,1]</h2>

<h2 id="1-30">[1,]   30</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}
X %*% a
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-2">[,1]</h2>

<h2 id="1-30-1">[1,]   30</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}</p>

<h1 id="what-about-when-m-5">What about when m = 5</h1>

<p>X &lt;- matrix(rep(1,10), nrow = 5, ncol = 2)
t(a) %*% X
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="error-in-t-a-x-non-conformable-arguments">Error in t(a) %*% X: non-conformable arguments</h2>

<p>{% endhighlight %}</p>

<p>{% highlight r %}</p>

<h1 id="what-about-x-alpha">What about X %*% alpha</h1>

<p>X %*% a
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-3">[,1]</h2>

<h2 id="1-30-2">[1,]   30</h2>

<h2 id="2-30">[2,]   30</h2>

<h2 id="3-30">[3,]   30</h2>

<h2 id="4-30">[4,]   30</h2>

<h2 id="5-30">[5,]   30</h2>

<p>{% endhighlight %}</p>

<h4 id="why-is-this-relevant-to-neural-networks">Why is this relevant to neural networks?</h4>

<p>In the neural network example that I have dealt with so far, we use the logistic function as the activation function for our network, which is essentially linear regression pushed through a sigmoid function.
We use the same $\alpha^{T}X$, which must include the bias terms $x_{m,0} = 1$.</p>

<p>$$
h(\alpha) = \dfrac{1}{1+e^{-\alpha^TX}}
$$</p>

<p>This rings true for the other non-linearities, like softmax, which I&rsquo;ll come back to in my next post.</p>

<h2 id="references">References</h2>

<p>Hastie, T., Tibshirani, R., and Friedman, J. (2009). <em>The Elements of Statistical Learning (Second Edition)</em>. Springer-Verlag. 763 pages.</p>

<p>{% highlight r %}
devtools::session_info()
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="setting-value">setting  value</h2>

<h2 id="version-r-version-3-3-2-2016-10-31">version  R version 3.3.2 (2016-10-31)</h2>

<h2 id="system-x86-64-linux-gnu">system   x86_64, linux-gnu</h2>

<h2 id="ui-rstudio-1-0-44">ui       RStudio (1.0.44)</h2>

<h2 id="language-en-gb-en">language en_GB:en</h2>

<h2 id="collate-en-gb-utf-8">collate  en_GB.UTF-8</h2>

<h2 id="tz-gb">tz       GB</h2>

<h2 id="date-2017-01-14">date     2017-01-14</h2>

<h2 id="package-version-date">package      * version  date</h2>

<h2 id="algdesign-1-1-7-3-2014-10-15">AlgDesign      1.1-7.3  2014-10-15</h2>

<h2 id="assertthat-0-1-2013-12-06">assertthat     0.1      2013-12-06</h2>

<h2 id="car-2-1-4-2016-12-02">car            2.1-4    2016-12-02</h2>

<h2 id="caret-6-0-73-2016-11-10">caret        * 6.0-73   2016-11-10</h2>

<h2 id="checkpoint-0-3-18-2016-10-31">checkpoint     0.3.18   2016-10-31</h2>

<h2 id="coda-0-18-1-2015-10-16">coda           0.18-1   2015-10-16</h2>

<h2 id="codetools-0-2-15-2016-10-05">codetools      0.2-15   2016-10-05</h2>

<h2 id="colorspace-1-3-1-2016-11-18">colorspace     1.3-1    2016-11-18</h2>

<h2 id="data-table-1-9-8-2016-11-25">data.table     1.9.8    2016-11-25</h2>

<h2 id="dbi-0-5-1-2016-09-10">DBI            0.5-1    2016-09-10</h2>

<h2 id="devtools-1-12-0-2016-06-24">devtools       1.12.0   2016-06-24</h2>

<h2 id="digest-0-6-10-2016-08-02">digest         0.6.10   2016-08-02</h2>

<h2 id="dplyr-0-5-0-2016-06-24">dplyr        * 0.5.0    2016-06-24</h2>

<h2 id="emoa-0-5-0-2012-09-25">emoa           0.5-0    2012-09-25</h2>

<h2 id="estimability-1-2-2016-11-19">estimability   1.2      2016-11-19</h2>

<h2 id="evaluate-0-10-2016-10-11">evaluate       0.10     2016-10-11</h2>

<h2 id="foreach-1-4-3-2015-10-13">foreach        1.4.3    2015-10-13</h2>

<h2 id="ggally-1-3-0-2016-11-13">GGally       * 1.3.0    2016-11-13</h2>

<h2 id="ggplot2-2-2-0-2016-11-11">ggplot2      * 2.2.0    2016-11-11</h2>

<h2 id="ggthemes-3-3-0-2016-11-24">ggthemes     * 3.3.0    2016-11-24</h2>

<h2 id="govstyle-0-1-1-2016-12-17">govstyle     * 0.1.1    2016-12-17</h2>

<h2 id="gtable-0-2-0-2016-02-26">gtable         0.2.0    2016-02-26</h2>

<h2 id="highr-0-6-2016-05-09">highr          0.6      2016-05-09</h2>

<h2 id="iterators-1-0-8-2015-10-13">iterators      1.0.8    2015-10-13</h2>

<h2 id="knitr-1-15-1-2016-11-22">knitr          1.15.1   2016-11-22</h2>

<h2 id="labeling-0-3-2014-08-23">labeling       0.3      2014-08-23</h2>

<h2 id="lattice-0-20-34-2016-09-06">lattice      * 0.20-34  2016-09-06</h2>

<h2 id="lazyeval-0-2-0-2016-06-12">lazyeval       0.2.0    2016-06-12</h2>

<h2 id="lme4-1-1-12-2016-04-16">lme4           1.1-12   2016-04-16</h2>

<h2 id="lsmeans-2-25-2016-11-19">lsmeans        2.25     2016-11-19</h2>

<h2 id="magrittr-1-5-2014-11-22">magrittr     * 1.5      2014-11-22</h2>

<h2 id="mass-7-3-45-2015-11-10">MASS           7.3-45   2015-11-10</h2>

<h2 id="matrix-1-2-7-1-2016-09-01">Matrix         1.2-7.1  2016-09-01</h2>

<h2 id="matrixmodels-0-4-1-2015-08-22">MatrixModels   0.4-1    2015-08-22</h2>

<h2 id="mco-1-0-15-1-2014-11-29">mco            1.0-15.1 2014-11-29</h2>

<h2 id="memoise-1-0-0-2016-01-29">memoise        1.0.0    2016-01-29</h2>

<h2 id="mgcv-1-8-16-2016-11-07">mgcv           1.8-16   2016-11-07</h2>

<h2 id="minqa-1-2-4-2014-10-09">minqa          1.2.4    2014-10-09</h2>

<h2 id="modelmetrics-1-1-0-2016-08-26">ModelMetrics   1.1.0    2016-08-26</h2>

<h2 id="multcomp-1-4-6-2016-07-14">multcomp       1.4-6    2016-07-14</h2>

<h2 id="munsell-0-4-3-2016-02-13">munsell        0.4.3    2016-02-13</h2>

<h2 id="mvtnorm-1-0-5-2016-02-02">mvtnorm        1.0-5    2016-02-02</h2>

<h2 id="nlme-3-1-128-2016-05-10">nlme           3.1-128  2016-05-10</h2>

<h2 id="nloptr-1-0-4-2014-08-04">nloptr         1.0.4    2014-08-04</h2>

<h2 id="nnet-7-3-12-2016-02-02">nnet           7.3-12   2016-02-02</h2>

<h2 id="pbkrtest-0-4-6-2016-01-27">pbkrtest       0.4-6    2016-01-27</h2>

<h2 id="plyr-1-8-4-2016-06-08">plyr           1.8.4    2016-06-08</h2>

<h2 id="prettyunits-1-0-2-2015-07-13">prettyunits    1.0.2    2015-07-13</h2>

<h2 id="progress-1-1-2-2016-12-14">progress       1.1.2    2016-12-14</h2>

<h2 id="purrr-0-2-2-2016-06-18">purrr        * 0.2.2    2016-06-18</h2>

<h2 id="quantreg-5-29-2016-09-04">quantreg       5.29     2016-09-04</h2>

<h2 id="r6-2-2-0-2016-10-05">R6             2.2.0    2016-10-05</h2>

<h2 id="randomforest-4-6-12-2015-10-07">randomForest   4.6-12   2015-10-07</h2>

<h2 id="rcolorbrewer-1-1-2-2014-12-07">RColorBrewer   1.1-2    2014-12-07</h2>

<h2 id="rcpp-0-12-8-2016-11-17">Rcpp           0.12.8   2016-11-17</h2>

<h2 id="readr-1-0-0-2016-08-03">readr        * 1.0.0    2016-08-03</h2>

<h2 id="reshape-0-8-6-2016-10-21">reshape        0.8.6    2016-10-21</h2>

<h2 id="reshape2-1-4-2-2016-10-22">reshape2       1.4.2    2016-10-22</h2>

<h2 id="rgp-0-4-1-2014-08-08">rgp          * 0.4-1    2014-08-08</h2>

<h2 id="rmd2md-0-1-2-2016-10-23">rmd2md       * 0.1.2    2016-10-23</h2>

<h2 id="rpart-4-1-10-2015-06-29">rpart          4.1-10   2015-06-29</h2>

<h2 id="rsm-2-8-2016-10-16">rsm            2.8      2016-10-16</h2>

<h2 id="rstudioapi-0-6-2016-06-27">rstudioapi     0.6      2016-06-27</h2>

<h2 id="sandwich-2-3-4-2015-09-24">sandwich       2.3-4    2015-09-24</h2>

<h2 id="scales-0-4-1-2016-11-09">scales         0.4.1    2016-11-09</h2>

<h2 id="sparsem-1-74-2016-11-10">SparseM        1.74     2016-11-10</h2>

<h2 id="spot-1-1-0-2016-06-09">SPOT         * 1.1.0    2016-06-09</h2>

<h2 id="stringi-1-1-2-2016-10-01">stringi        1.1.2    2016-10-01</h2>

<h2 id="stringr-1-1-0-2016-08-19">stringr        1.1.0    2016-08-19</h2>

<h2 id="survival-2-39-4-2016-05-11">survival       2.39-4   2016-05-11</h2>

<h2 id="th-data-1-0-7-2016-01-28">TH.data        1.0-7    2016-01-28</h2>

<h2 id="tibble-1-2-2016-08-26">tibble       * 1.2      2016-08-26</h2>

<h2 id="tidyr-0-6-0-2016-08-12">tidyr        * 0.6.0    2016-08-12</h2>

<h2 id="tidyverse-1-0-0-2016-09-09">tidyverse    * 1.0.0    2016-09-09</h2>

<h2 id="withr-1-0-2-2016-06-20">withr          1.0.2    2016-06-20</h2>

<h2 id="xgboost-0-6-4-2017-01-05">xgboost      * 0.6-4    2017-01-05</h2>

<h2 id="xtable-1-8-2-2016-02-05">xtable         1.8-2    2016-02-05</h2>

<h2 id="zoo-1-7-13-2016-05-03">zoo            1.7-13   2016-05-03</h2>

<h2 id="source">source</h2>

<h2 id="cran-r-3-3-2">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-1">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-2">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-3">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-4">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-1">CRAN (R 3.3.1)</h2>

<h2 id="cran-r-3-3-2-5">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-6">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-1">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-7">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-2">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-2-3-3">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-8">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-9">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-10">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-4">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-11">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-12">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-13">CRAN (R 3.3.2)</h2>

<h2 id="github-ukgovdatascience-govstyle-2ef672b">Github (ukgovdatascience/govstyle@2ef672b)</h2>

<h2 id="cran-r-3-2-3-5">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-2-3-6">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-2-3-7">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-14">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-8">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-1-1">CRAN (R 3.3.1)</h2>

<h2 id="cran-r-3-3-2-15">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-16">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-17">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-9">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-2-5">CRAN (R 3.2.5)</h2>

<h2 id="cran-r-3-3-1-2">CRAN (R 3.3.1)</h2>

<h2 id="cran-r-3-3-2-18">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-19">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-10">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-20">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-21">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-22">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-23">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-11">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-24">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-1-3">CRAN (R 3.3.1)</h2>

<h2 id="cran-r-3-3-2-25">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-5-1">CRAN (R 3.2.5)</h2>

<h2 id="cran-r-3-3-2-26">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-12">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-27">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-28">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-13">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-29">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-14">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-30">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-15">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-31">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-16">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-32">CRAN (R 3.3.2)</h2>

<h2 id="cran-1-4-2">cran (@1.4.2)</h2>

<h2 id="cran-r-3-3-2-33">CRAN (R 3.3.2)</h2>

<h2 id="github-ivyleavedtoadflax-rmd2md-3fa6541">Github (ivyleavedtoadflax/rmd2md@3fa6541)</h2>

<h2 id="cran-r-3-2-1">CRAN (R 3.2.1)</h2>

<h2 id="cran-r-3-3-2-34">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-17">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-35">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-36">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-37">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-3-2-38">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-18">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-2-3-19">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-1-4">CRAN (R 3.3.1)</h2>

<h2 id="cran-r-3-3-2-39">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-20">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-2-3-21">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-2-3-22">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-2-3-23">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-40">CRAN (R 3.3.2)</h2>

<h2 id="cran-r-3-2-3-24">CRAN (R 3.2.3)</h2>

<h2 id="cran-r-3-3-2-41">CRAN (R 3.3.2)</h2>

<p>{% endhighlight %}</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

