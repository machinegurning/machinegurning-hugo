<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.35" />


<title>Unsupervised learning with K-means - A Hugo website</title>
<meta property="og:title" content="Unsupervised learning with K-means - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">10 min read</span>
    

    <h1 class="article-title">Unsupervised learning with K-means</h1>

    
    <span class="article-date">2015/11/11</span>
    

    <div class="article-content">
      

<p><em>K</em> means is one of the simplest algorithms for unsupervised learning.
It works iteratively to cluster together similar values in a dataset of one to many dimensions ($X\in\mathbb{R}^{m \times n}$).
The algorithmic steps are simple, it relies only on arithmetic means, so it&rsquo;s pretty simple to understand, but can also be quite powerful.
Because it relies on random sampling to initiate the algorithm it can be quite slow however, as there is a need to complete many replications to get a robust result.</p>

<p><em>K</em>-means features as an exercise in <a href="http://www.coursera.com">Coursera</a>&rsquo;s machine learning course, which I am working through.
In this post I produce a simple implementation of the <em>K</em> means algorithm in R, and because I&rsquo;m trying to improve my package writing, I wrap it up into a simple package called <a href="https://github.com/ivyleavedtoadflax/knn">knn</a>.</p>

<h3 id="the-problem">The problem</h3>

<p><em>K</em>-means can be divided into two steps.
In a two-dimensional example, given a dataset $X\in\mathbb{R}^{m \times 2}$, the first step is to calculate the closest centroid for each training example $x$.
In my implementation, centroids $\{1,&hellip;,K\}$ can be assigned manually, or are automatically generated by selecting $K$ random training examples, and setting these as the centroids.</p>

<p>Mathematically, for each training example $x^{i}$ we are minimising</p>

<p>$$
||x^{i}-\mu_j||^{2}
$$</p>

<p>i.e. calculating the Euclidean norm of the vector $x^{(i)}$.
This is very simply achieved through pythagorean trigonometry, and is already implemented in R with <code>norm</code>.</p>

<h4 id="generate-dummy-data">Generate dummy data</h4>

<p>The first step is to generate dummy two dimensional data which will demonstrate the principles well.
It&rsquo;s expedient to functionalise this.</p>

<p>{% highlight r %}
set.seed(1337)
dummy_group &lt;- function(x = 30, mean = 10, sd = 2) {</p>

<p>cbind(
    rnorm(x, mean, sd),
    rnorm(x, mean, sd)
  )</p>

<p>}</p>

<p>X &lt;- rbind(
  dummy_group(mean = 10),
  dummy_group(mean = 20),
  dummy_group(mean = 30)
)
{% endhighlight %}</p>

<p>Which gives us a small dataset with some clear clusters to work with.</p>

<p>{% highlight r %}
plot(
  X,
  xlab = expression(x[1]),
  ylab = expression(x[2])
)
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-plot-clusters-1.png"><img src="/figures/2015-11-11-plot-clusters-1.png" alt="plot of chunk 2015-11-11-plot-clusters" /></a></p>

<h4 id="calculating-norms">Calculating norms</h4>

<p>Calculating norms can be done with base R using <code>norm</code>.</p>

<p>{% highlight r %}</p>

<h2 id="note-norm-only-accepts-a-matrix">Note norm only accepts a matrix</h2>

<p>norm(as.matrix(c(2,4)),type = &ldquo;f&rdquo;)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-4-472136">[1] 4.472136</h2>

<p>{% endhighlight %}</p>

<p>But, the <code>norm</code> function isn&rsquo;t too useful in the present case, as we really want to vectorise this process, so I use my own function <code>rowNorms</code>:</p>

<p>{% highlight r %}</p>

<h2 id="define-rownorms">Define rowNorms()</h2>

<p>rowNorms &lt;- function(x) {</p>

<p>norms &lt;- sqrt(rowSums(x ^ 2))
return(norms)</p>

<p>}
{% endhighlight %}</p>

<p><code>rowNorms</code>  works across a matrix, and will return a vector of norms, whilst <code>norm</code> will return the norm of the matrix.</p>

<p>{% highlight r %}
rowNorms(X) %&gt;% str
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="num-1-90-16-3-14-1-12-1-21-5-12-1">num [1:90] 16.3 14.1 12.1 21.5 12.1 &hellip;</h2>

<p>{% endhighlight %}</p>

<p>This function is implemented in the [<code>knn</code>]() package.</p>

<h4 id="calculate-distances">Calculate distances</h4>

<p>So now I can produce a vector of euclidean norms, I can put this into practice for calculating the distance between points $x^i$ and the initial centroids.</p>

<p>In the first instance I will specify some centroids, although as noted, more usual practice is to randomly initiate these from $X$.</p>

<p>So to calculate the distance between some pre-specified centroids and $X$:</p>

<p>{% highlight r %}</p>

<h2 id="first-create-a-matrix-of-initial-centroids">First create a matrix of initial centroids</h2>

<p>centroids &lt;- rbind(
c(15, 15),
c(25, 15),
c(25, 35)
)</p>

<h2 id="create-a-matrix-with-m-rows-consisting-entirely-of-the-first-centroid">Create a matrix with m rows, consisting entirely of the first centroid.</h2>

<p>first_centroid &lt;- matrix(
  rep(centroids[1,], nrow(X)),
  ncol = 2,
  byrow = TRUE
  )</p>

<p>diff &lt;- X - first_centroid</p>

<p>distances &lt;- rowNorms(diff)</p>

<p>distances %&gt;% str
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="num-1-90-5-24-8-38-9-19-2-58-9-06">num [1:90] 5.24 8.38 9.19 2.58 9.06 &hellip;</h2>

<p>{% endhighlight %}</p>

<p>Rather than repeating this for all three initial centroids, I&rsquo;ve wrapped this up into a function.
Here <code>knn::find_group()</code> calculates the closest centroid and returns this as a vector of values ${1,..,K}$, where $K$ is the number of centroids.</p>

<p>{% highlight r %}
groups &lt;- find_group(X, centroids = centroids)</p>

<h2 id="returns-a-vector-of-groups-based-on-proximity-to-the-initial-centroids">Returns a vector of groups based on proximity to the initial centroids</h2>

<p>groups %&gt;% str
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="int-1-90-1-1-1-1-1-1-1-1-1-1">int [1:90] 1 1 1 1 1 1 1 1 1 1 &hellip;</h2>

<p>{% endhighlight %}</p>

<h4 id="calculate-centroids">Calculate centroids</h4>

<p>The next step of the process is to now calculate the new centroid from the groups that were established in the previous step.</p>

<p>Again, this is qute a simple thing to code.
I achieved it by sequentially subsetting $X$ by $k$, and outputting the new centroids in a matrix  $Y\in\mathbb{R}^{K \times n}$.
I&rsquo;ve wrapped this step into a function called <code>centroid_mean()</code>.</p>

<p>{% highlight r %}
Y &lt;- centroid_mean(X, groups)
{% endhighlight %}</p>

<p>With the centroids calculated, now is a good chance to plot again, and check the sanity of the new centroids.</p>

<p>I&rsquo;ve written a function <code>plot_knn()</code> to do this (it would make sense to roll this up into a plot method one day&hellip;).</p>

<p>Start with the original centroids&hellip;</p>

<p>{% highlight r %}
plot_knn(X, centroids)
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-first-plot-1.png"><img src="/figures/2015-11-11-first-plot-1.png" alt="plot of chunk 2015-11-11-first-plot" /></a></p>

<p>And now with the new centroids&hellip;</p>

<p>{% highlight r %}
plot_knn(X, Y)
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-new-centroids-1.png"><img src="/figures/2015-11-11-new-centroids-1.png" alt="plot of chunk 2015-11-11-new-centroids" /></a></p>

<p>Not bad, so it looks things are moving in the right direction, and with one further iteration, it looks like we are pretty close to the original centroids.</p>

<p>{% highlight r %}
groups1 &lt;- find_group(X, centroids = Y)
Y1 &lt;- centroid_mean(X, groups1)
plot_knn(X, Y1)
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-groups1-1.png"><img src="/figures/2015-11-11-groups1-1.png" alt="plot of chunk 2015-11-11-groups1" /></a></p>

<p>I could keep on going manually, but I have already implemented the whole process in the function <code>knn()</code>. So I&rsquo;ll try that instead, this time randomly initiating the centroids.</p>

<p>{% highlight r %}</p>

<h1 id="set-seed-to-make-results-reproducible">Set seed to make results reproducible</h1>

<p>set.seed(1)
foo &lt;- knn(X, k = 3)
plot_knn(X, foo$centroids, foo$groups)
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-knn-1.png"><img src="/figures/2015-11-11-knn-1.png" alt="plot of chunk 2015-11-11-knn" /></a></p>

<p>Great, so what are the final centroids?</p>

<p>{% highlight r %}
foo$centroids %&gt;% round
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-2">[,1] [,2]</h2>

<h2 id="1-11-10">[1,]   11   10</h2>

<h2 id="2-30-30">[2,]   30   30</h2>

<h2 id="3-20-20">[3,]   20   20</h2>

<p>{% endhighlight %}</p>

<p>&hellip;which as you&rsquo;ll remember were the starting values about which I generated the normal distributions.
Of course, it wouldn&rsquo;t normally be possible to assess the success of the algorithm so easily, and note that the order of the centroids has changed, although this doesn&rsquo;t matter.</p>

<h3 id="a-harder-test">A harder test</h3>

<p>I&rsquo;ll try again with a more difficult example.
Here I am still relying on three normally distributed clusters about the same centroids, but I specify a much greater standard deviation, so it is much more difficult with the human eye alone to separate the clusters.
I&rsquo;ll specify $x = 100$ so that the algorithm has more data to work with.</p>

<p>{% highlight r %}
X &lt;- rbind(
  dummy_group(x = 100, mean = 10, sd = 8),
  dummy_group(x = 100, mean = 20, sd = 8),
  dummy_group(x = 100, mean = 30, sd = 8)
)</p>

<p>plot(
  X,
  xlab = expression(x[1]),
  ylab = expression(x[2])
    )
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-harder-test-1.png"><img src="/figures/2015-11-11-harder-test-1.png" alt="plot of chunk 2015-11-11-harder-test" /></a></p>

<p>Hard to see three distinct clusters&hellip;now what the algorithm makes of it.</p>

<p>Again, I use the <code>knn()</code> function which takes the leg work out of the programming.</p>

<p>In the example below I also set a seed which will make the results reproducible.
As I noted in the pre-amble, the solutions from <em>K</em>-means are dependent on the initial centroids, so it is usual practice to repeat the process many times, and then choose the group that has the highest probability.</p>

<p>I&rsquo;ve not yet implemented this repetition in <code>knn</code> yet (and probably won&rsquo;t go that far), so for now I will use <code>set.seed()</code> to demonstrate the ideal and not-so-ideal outcomes.</p>

<p>{% highlight r %}</p>

<h2 id="set-seed-2-for-a-good-outcome">set.seed(2) for a good outcome</h2>

<p>set.seed(2)
foo &lt;- knn(X)</p>

<p>plot_knn(X, foo$centroids, foo$groups)
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-good-outcome-1.png"><img src="/figures/2015-11-11-good-outcome-1.png" alt="plot of chunk 2015-11-11-good-outcome" /></a></p>

<p>{% highlight r %}
foo$centroids %&gt;% round
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-2-1">[,1] [,2]</h2>

<h2 id="1-9-7">[1,]    9    7</h2>

<h2 id="2-33-29">[2,]   33   29</h2>

<h2 id="3-18-21">[3,]   18   21</h2>

<p>{% endhighlight %}</p>

<p>So we&rsquo;re a little bit out this time after 3 steps, but not too far.
This could probably be improved by setting a more precise endpoint, which at present has a very simplistic implementation.</p>

<p>So what happens if I change the seed?</p>

<p>{% highlight r %}
set.seed(100)
bar &lt;- knn(X)
plot_knn(X, bar$centroids, bar$groups)
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-bad-outcome-1.png"><img src="/figures/2015-11-11-bad-outcome-1.png" alt="plot of chunk 2015-11-11-bad-outcome" /></a></p>

<p>{% highlight r %}
bar$centroids %&gt;% round
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-2-2">[,1] [,2]</h2>

<h2 id="1-27-24">[1,]   27   24</h2>

<h2 id="2-9-5">[2,]    9    5</h2>

<h2 id="3-9-21">[3,]    9   21</h2>

<p>{% endhighlight %}</p>

<p>So the solution given by the algorithm has changed, and this highlights one of the issues with <em>K</em>-means: it can be very dependent on the initial location of centroids.
This is why it is usual to repeat the process many times, then essentially average the results.
I haven&rsquo;t gone that far with my simple implementation, but the in-built implementation in base R (<code>kmeans</code>) does include an argument <code>nstarts</code> which will give you $n$ possible scenarios.</p>

<p>So how well does <code>kmeans()</code> perform in the current scenario? Well not vastly different it turns out.</p>

<p>{% highlight r %}
foobar &lt;- kmeans(X, centers = 3, nstart = 1)
plot_knn(X, foobar$centers, foobar$cluster)
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-base-kmeans-1.png"><img src="/figures/2015-11-11-base-kmeans-1.png" alt="plot of chunk 2015-11-11-base-kmeans" /></a></p>

<p>{% highlight r %}
foobar$centers %&gt;% round
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-2-3">[,1] [,2]</h2>

<h2 id="1-9-7-1">1    9    7</h2>

<h2 id="2-33-29-1">2   33   29</h2>

<h2 id="3-18-22">3   18   22</h2>

<p>{% endhighlight %}</p>

<p>What about the improvement with additional replications?</p>

<p>{% highlight r %}
foobar &lt;- kmeans(X, centers = 3, nstart = 1000)
plot_knn(X, foobar$centers, foobar$cluster)
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-base-kmeans-replication-1.png"><img src="/figures/2015-11-11-base-kmeans-replication-1.png" alt="plot of chunk 2015-11-11-base-kmeans-replication" /></a></p>

<p>{% highlight r %}
foobar$centers %&gt;% round
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="1-2-4">[,1] [,2]</h2>

<h2 id="1-9-6">1    9    6</h2>

<h2 id="2-18-21">2   18   21</h2>

<h2 id="3-33-29">3   33   29</h2>

<p>{% endhighlight %}</p>

<p>At two decimal places, there is only a small difference between the final centroids, and those produced by only a single repetition using <code>kmeans()</code>.
Of course I should note that <code>kmeans()</code> uses a more complicated (and more accurate) algorithm than the simplified implementation I have used in <a href="https://github.com/ivyleavedtoadflax/knn"><code>knn</code></a>.</p>

<h4 id="a-more-interesting-application">A more interesting application</h4>

<p>One of the ways that <em>K</em>-means can be used is for image compression.
Images are just arrays of values, which code for intensity.
A grayscale image requires just a single matrix, while a colour image typically requires three matrices.
By clustering similar colour values in an image, we can compress an image by reducing the number of colours required to render it.
Note that png images may also have a fourth channel which codes for alpha, or transparency.</p>

<p>I use the [jpeg]() package to load and image, in an example inspired by both <a href="http://www.magesblog.com/2012/12/now-i-see-it-k-means-cluster-analysis.html">Markus Gesmann</a> and the Coursera machine learning course, but using my own implementation of <em>K</em>-means.</p>

<p>First I load in a holiday snap in glorious full colour&hellip;</p>

<p>{% highlight r %}
library(jpeg)</p>

<p>img &lt;- readJPEG(source = &ldquo;figures/2015-11-11-knn.jpeg&rdquo;)</p>

<h2 id="get-dimensions-of-image">Get dimensions of image</h2>

<p>dm &lt;- dim(img)</p>

<h2 id="create-a-matrix-of-x-and-y-pixel-positions-and-intensity-values-and-convert">Create a matrix of x and y pixel positions and intensity values, and convert</h2>

<h2 id="into-rgb">into rgb</h2>

<p>img_df &lt;- data_frame(
  x = rep(1:dm[2], each = dm[1]),
  y = rep(dm[1]:1, dm[2]),
  red = as.vector(img[,,1]),
  green = as.vector(img[,,2]),
  blue = as.vector(img[,,3]),
  cols = rgb(red, green, blue)
)</p>

<h2 id="plot-the-original-image">Plot the original image</h2>

<p>plot(
  y ~ x,
  data = img_df,
  main = &ldquo;Venice canal (original)&rdquo;,
  col = cols,
  pch = &ldquo;.&rdquo;,
  axes = FALSE,
  xlab = &ldquo;&rdquo;,
  ylab = &ldquo;&rdquo;
)
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-256-colors-1.png"><img src="/figures/2015-11-11-256-colors-1.png" alt="plot of chunk 2015-11-11-256-colors" /></a></p>

<p>Now the same image again after clustering the colours into sixteen clusters.</p>

<p>{% highlight r %}</p>

<h2 id="get-clusters-from-the-three-intensity-values">Get clusters from the three intensity values.</h2>

<p>kmeans16 &lt;- img_df %&gt;%
  select(red:blue) %&gt;%
  knn(k = 16)</p>

<h2 id="convert-clusters-into-rgb-values-and-append-as-column-to-img-df">Convert clusters into rgb values, and append as column to img_df</h2>

<p>img_df$cols16 &lt;- rgb(kmeans16$centroids[kmeans16$groups,])</p>

<h2 id="plot-the-image-again-this-time-using-the-new-clustered-colours">Plot the image again, this time using the new clustered colours.</h2>

<p>plot(
  y ~ x,
  data = img_df,
  main = &ldquo;Venice canal (16 colours)&rdquo;,
  col = cols16,
  pch = &ldquo;.&rdquo;,
  axes = FALSE,
  xlab = &ldquo;&rdquo;,
  ylab = &ldquo;&rdquo;
)
{% endhighlight %}</p>

<p><a href="/figures/2015-11-11-16-colors-1.png"><img src="/figures/2015-11-11-16-colors-1.png" alt="plot of chunk 2015-11-11-16-colors" /></a></p>

<p>So it works!
And whilst I have not attempted to calculate the resulting image compression, the reduction in possible colour values should lead to a pretty considerable reduction in the file size.</p>

<p>I&rsquo;m not going to develop this package any further, because far better implementations are already out there, but it was a good exercise to learn more about <em>K</em>-means, and package writing in R.</p>

<p>{% highlight r %}
sessionInfo()
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="r-version-3-2-3-2015-12-10">R version 3.2.3 (2015-12-10)</h2>

<h2 id="platform-x86-64-pc-linux-gnu-64-bit">Platform: x86_64-pc-linux-gnu (64-bit)</h2>

<h2 id="running-under-ubuntu-14-04-3-lts">Running under: Ubuntu 14.04.3 LTS</h2>

<h2 id="locale">locale:</h2>

<h2 id="1-lc-ctype-en-gb-utf-8-lc-numeric-c">[1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C</h2>

<h2 id="3-lc-time-en-gb-utf-8-lc-collate-en-gb-utf-8">[3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8</h2>

<h2 id="5-lc-monetary-en-gb-utf-8-lc-messages-en-gb-utf-8">[5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8</h2>

<h2 id="7-lc-paper-en-gb-utf-8-lc-name-c">[7] LC_PAPER=en_GB.UTF-8       LC_NAME=C</h2>

<h2 id="9-lc-address-c-lc-telephone-c">[9] LC_ADDRESS=C               LC_TELEPHONE=C</h2>

<h2 id="11-lc-measurement-en-gb-utf-8-lc-identification-c">[11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C</h2>

<h2 id="attached-base-packages">attached base packages:</h2>

<h2 id="1-methods-stats-graphics-grdevices-utils-datasets-base">[1] methods   stats     graphics  grDevices utils     datasets  base</h2>

<h2 id="other-attached-packages">other attached packages:</h2>

<h2 id="1-knn-0-1-3-jpeg-0-1-8-dplyr-0-4-3-testthat-0-11-0">[1] knn_0.1.3       jpeg_0.1-8      dplyr_0.4.3     testthat_0.11.0</h2>

<h2 id="5-knitr-1-11">[5] knitr_1.11</h2>

<h2 id="loaded-via-a-namespace-and-not-attached">loaded via a namespace (and not attached):</h2>

<h2 id="1-rcpp-0-12-2-digest-0-6-8-crayon-1-3-1-assertthat-0-1">[1] Rcpp_0.12.2     digest_0.6.8    crayon_1.3.1    assertthat_0.1</h2>

<h2 id="5-r6-2-1-1-dbi-0-3-1-formatr-1-2-1-magrittr-1-5">[5] R6_2.1.1        DBI_0.3.1       formatR_1.2.1   magrittr_1.5</h2>

<h2 id="9-evaluate-0-8-stringi-1-0-1-lazyeval-0-1-10-tools-3-2-3">[9] evaluate_0.8    stringi_1.0-1   lazyeval_0.1.10 tools_3.2.3</h2>

<h2 id="13-stringr-1-0-0-parallel-3-2-3-memoise-0-2-1">[13] stringr_1.0.0   parallel_3.2.3  memoise_0.2.1</h2>

<p>{% endhighlight %}</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

