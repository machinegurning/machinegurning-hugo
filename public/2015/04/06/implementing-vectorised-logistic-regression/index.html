<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.35" />


<title>Implementing vectorised logistic regression - A Hugo website</title>
<meta property="og:title" content="Implementing vectorised logistic regression - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">6 min read</span>
    

    <h1 class="article-title">Implementing vectorised logistic regression</h1>

    
    <span class="article-date">2015/04/06</span>
    

    <div class="article-content">
      

<p>I&rsquo;ve been doing Andrew Ng&rsquo;s excellent Machine Learning course on <a href="www.coursera.org">coursera</a>. The second exercise is to implement from scratch vectorised logistic regression for classification. Submissions to the exercises have to be made in Octave or Matlab; in this post I give the solution using R.</p>

<p>Andrew Ng uses the algorithm <code>fminunc</code> in Matlab/Octave to optimise the logistic regression solution. In R you can use the <code>optim</code> function, but I have been using the <code>ucminf</code> function provided in the package <code>ucminf</code>. <code>uncminf</code> takes the following arguments:</p>

<p><code>ucminf(par, fn, gr = NULL, ..., control = list(), hessian=0)</code></p>

<p>The ones we are interested in are:</p>

<table>
<thead>
<tr>
<th>Arguments</th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td><code>par</code></td>
<td>Initial estimate of minimum for fn.</td>
</tr>

<tr>
<td><code>fn</code></td>
<td>Objective function to be minimized.</td>
</tr>

<tr>
<td><code>gr</code></td>
<td>Gradient of objective function If <code>NULL</code> a finite difference approximation is used.</td>
</tr>
</tbody>
</table>

<p>So I need to define three functions: logistic regression, a cost function, and a function which returns the gradient of that cost. These are defined in the course, helpfully:</p>

<p>Logistic regression is defined as:</p>

<p>$$
h_{\theta}(x)=g(\theta^{T}x)
$$</p>

<p>where $g$ is the sigmoid function:</p>

<p>$$
g(z)=\frac{1}{1+e^{-z}}
$$</p>

<p>The cost function is given by:</p>

<p>$$
J(\theta)=\frac{1}{m}\sum^m<em>{i=1}[-y^{(i)}\log(h</em>\theta(x^{(i)}))-(1-y^{(i)})\log(1-h_\theta(x^{(i)}))]
$$</p>

<p>And the gradient of the cost is a vector of the same length as $\theta$ where the $j^{th}$ element (for $j = 0,1,\cdots,n$) is defined as:</p>

<p>$$
\frac{\delta J(\theta)}{\delta\theta<em>{j}}=\frac{1}{m}\sum^{m}</em>{i=1}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j
$$</p>

<h3 id="vectorised-logistic-regression">Vectorised logistic regression</h3>

<p>The first step is to implement a sigmoid function:</p>

<p>{% highlight r %}
g &lt;- function(z) {</p>

<p>1 / (1 + exp(-z))</p>

<p>}</p>

<p>z &lt;- seq(-10,10,0.1)</p>

<p>plot(
  z,
  g(z),
  type = &ldquo;l&rdquo;
  )
{% endhighlight %}</p>

<p><a href="/figures/2015-04-06-sigmoid-function-1.png"><img src="/figures/2015-04-06-sigmoid-function-1.png" alt="plot of chunk 2015-04-06-sigmoid-function" /></a></p>

<p>&hellip;and with this function, implementing $h_{\theta}$ is easy:</p>

<p>{% highlight r %}
h &lt;- function(theta,X) {</p>

<p>g(X %*% theta)</p>

<p>}
{% endhighlight %}</p>

<h3 id="cost-function-and-gradient">Cost function and gradient</h3>

<p>I&rsquo;ll start by implementing an only partially vectorised version of the cost function $J(\theta)$:</p>

<p>{% highlight r %}
  J &lt;- function(X, y, theta) {
    (1/length(y)) * sum(-y*log(h(theta,X))-(1-y)*log(1-h(theta,X)))
    }
{% endhighlight %}</p>

<p>&hellip;and the gradient&hellip;</p>

<p>{% highlight r %}
gR &lt;- function(X,y,theta) {</p>

<p>error &lt;- h(theta,X) - y
  delta &lt;- t(X) %*% error / length(y)
  return(delta)</p>

<p>}
{% endhighlight %}</p>

<h3 id="testing-it-out">Testing it out&hellip;</h3>

<p>First, I&rsquo;ll plot the data:</p>

<p>{% highlight r %}
library(dplyr)
library(magrittr)
library(ggplot2)</p>

<p>ex2data1 &lt;- &ldquo;ex2data1.txt&rdquo; %&gt;%
  read.csv(header=FALSE) %&gt;%
  set_colnames(c(&ldquo;exam_score_1&rdquo;,&ldquo;exam_score_2&rdquo;,&ldquo;admitted&rdquo;))</p>

<p>p &lt;- ex2data1 %&gt;%
  ggplot(
    aes(
      x = exam_score_1,
      y = exam_score_2,
      shape = factor(admitted),
      colour = factor(admitted)
      )
    ) +
  geom_point()+
  xlab(&ldquo;Exam 1 score&rdquo;)+
  ylab(&ldquo;Exam 2 score&rdquo;)</p>

<p>p
{% endhighlight %}</p>

<p><a href="/figures/2015-04-06-log-reg-1.png"><img src="/figures/2015-04-06-log-reg-1.png" alt="plot of chunk 2015-04-06-log-reg" /></a></p>

<p>And now try out logistic regression with <code>ucminf</code>:</p>

<p>{% highlight r %}</p>

<h1 id="arrange-the-data-for-the-functions">arrange the data for the functions</h1>

<p>theta &lt;- matrix(c(0,0,0), ncol = 1)
X &lt;- ex2data1[,1:2] %&gt;% as.matrix %&gt;% cbind(1,.)
y &lt;- ex2data1[,3] %&gt;% as.matrix</p>

<p>library(ucminf)</p>

<p>ucminf_out &lt;- ucminf(
  par = theta,
  fn = function(t) J(X, y, t),
  gr = function(t) gR(X, y, t)
  )</p>

<p>ucminf_out
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="par">$par</h2>

<h2 id="1-25-1613327-0-2062317-0-2014716">[1] -25.1613327   0.2062317   0.2014716</h2>

<h2 id="value">$value</h2>

<h2 id="1-0-2034977">[1] 0.2034977</h2>

<h2 id="convergence">$convergence</h2>

<h2 id="1-1">[1] 1</h2>

<h2 id="message">$message</h2>

<h2 id="1-stopped-by-small-gradient-grtol">[1] &ldquo;Stopped by small gradient (grtol).&rdquo;</h2>

<h2 id="invhessian-lt">$invhessian.lt</h2>

<h2 id="1-3314-2043743-26-3783748-26-9990573-0-2247604-0-2016443">[1] 3314.2043743  -26.3783748  -26.9990573    0.2247604    0.2016443</h2>

<h2 id="6-0-2355011">[6]    0.2355011</h2>

<h2 id="info">$info</h2>

<h2 id="maxgradient-laststep-stepmax-neval">maxgradient     laststep      stepmax        neval</h2>

<h2 id="4-236716e-07-2-095353e-05-3-307500e-00-3-200000e-01">4.236716e-07 2.095353e-05 3.307500e+00 3.200000e+01</h2>

<p>{% endhighlight %}</p>

<p>So this gives a lot of output. But importantly it gives us three coefficients (<code>$par</code>), the final cost (<code>$value</code>), and that convergence was reached (<code>$convergence</code>).</p>

<p>Andrew Ng suggests that the final cost should be 0.203, which is what I get, so it seems to be working, and using <code>$par</code> to plot the decision voundary, we get a pretty good fit:</p>

<p>{% highlight r %}
theta &lt;- ucminf_out$par</p>

<p>boundary &lt;- function(x) {
  (-1/theta[3])*(theta[2]*x+theta[1])
  }</p>

<p>p + stat_function(
    fun = boundary,
    colour = &ldquo;black&rdquo;
    )
{% endhighlight %}</p>

<p><a href="/figures/2015-04-06-log-reg-boundary-1.png"><img src="/figures/2015-04-06-log-reg-boundary-1.png" alt="plot of chunk 2015-04-06-log-reg-boundary" /></a></p>

<h3 id="vectorising">Vectorising</h3>

<p>There is an excellent post on vectorising these functions on <a href="http://stackoverflow.com/questions/16700340/optimisation-in-r-using-ucminf-package">Stack Overflow</a> which gives a better vectorised version of the algorithms above, e.g.:</p>

<p>{% highlight r %}
Jv &lt;- function(X, y, theta) {
  -(1/length(y)) * crossprod(
    c(y, 1 - y),
    c(log(h(theta,X)), log(1 - h(theta,X)))
    )
  }
{% endhighlight %}</p>

<p>It wasn&rsquo;t immediately clear to me what&rsquo;s going on here, so I&rsquo;m going to break this down piece by piece.</p>

<p>First we create two vectors <code>c(y, 1 - y)</code> and <code>c(log(h(theta,X)), log(1 - h(theta,X)))</code> and compute the cross product of them. The first matrix is the concatenation of the $-y$ and $(1-y)$ terms for length $m$ from the equation:</p>

<p>$$J(\theta)\frac{1}{m}\sum^m<em>{i=1}[-y^{(i)}\log(h</em>\theta(x^{(i)}))-(1-y^{(i)})\log(1-h_\theta(x^{(i)}))]$$</p>

<p>The second vector concatenates the remaining terms:</p>

<p>$$\text{log}(h_{\theta(x^{(i)})})$$</p>

<p>and</p>

<p>$$\text{log}(1-h_{\theta(x^{(i)})})$$</p>

<p>The crossproduct of these two vectors is essentially the same as $\vec{a}^T\vec{b}$; basically the sum of every value of $\vec{a}$ multiplied by the corresponding value of $\vec{b}$. i.e.:  (note that not all of the first vector equal zero: $\vec{a_{(i)}}\neq0$, $\vec{a}\in{0,1}$).</p>

<p>$$
\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0
\end{bmatrix}\begin{bmatrix}
-0.6931472 <br />
-0.6931472 <br />
-0.6931472 <br />
\vdots <br />
-0.6931472
\end{bmatrix} = -0.6931472
$$</p>

<p>The $\delta$ function can also be speeded up slightly by employing <code>crossprod</code> instead of <code>t(X) %*% h(theta,X)</code>.</p>

<p>{% highlight r %}
gRv &lt;- function(X,y,theta) {</p>

<p>(1 / length(y)) * crossprod(X, h(theta,X) - y)</p>

<p>}
{% endhighlight %}</p>

<p>Ok so now that we have some additional vectorisation, let&rsquo;s look at plugging it into the <code>ucminf</code> function.</p>

<p>{% highlight r %}
ucminf_out_v &lt;- ucminf(
  par = theta,
  fn = function(t) Jv(X, y, t),
  gr = function(t) gRv(X, y, t)
  )</p>

<p>ucminf_out_v
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="par-1">$par</h2>

<h2 id="1-25-1613327-0-2062317-0-2014716-1">[1] -25.1613327   0.2062317   0.2014716</h2>

<h2 id="value-1">$value</h2>

<h2 id="1-0-2034977-1">[1] 0.2034977</h2>

<h2 id="convergence-1">$convergence</h2>

<h2 id="1-1-1">[1] 1</h2>

<h2 id="message-1">$message</h2>

<h2 id="1-stopped-by-small-gradient-grtol-1">[1] &ldquo;Stopped by small gradient (grtol).&rdquo;</h2>

<h2 id="invhessian-lt-1">$invhessian.lt</h2>

<h2 id="1-1-0-0-1-0-1">[1] 1 0 0 1 0 1</h2>

<h2 id="info-1">$info</h2>

<h2 id="maxgradient-laststep-stepmax-neval-1">maxgradient     laststep      stepmax        neval</h2>

<h2 id="4-236716e-07-0-000000e-00-3-307500e-00-1-000000e-00">4.236716e-07 0.000000e+00 3.307500e+00 1.000000e+00</h2>

<p>{% endhighlight %}</p>

<p>So great, the two are giving the same answer. But it would be interesting to see what the speed increase is like when comparing the non-vectorised, vectorised, and the usual <code>glm</code> method</p>

<p>First let me just check that the <code>glm</code> implementation returns the same parameters:</p>

<p>{% highlight r %}
model &lt;- glm(
  admitted ~ exam_score_1 + exam_score_2,
  family = &ldquo;binomial&rdquo;,
  data = ex2data1
  )</p>

<p>coef(model)
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="intercept-exam-score-1-exam-score-2">(Intercept) exam_score_1 exam_score_2</h2>

<h2 id="25-1613335-0-2062317-0-2014716">-25.1613335    0.2062317    0.2014716</h2>

<p>{% endhighlight %}</p>

<p>Perfect. Now to compare the three I&rsquo;ll use the excellent <a href="ttp://CRAN.R-project.org/package=rbenchmark"><code>rbenchmark</code></a> package.</p>

<p>{% highlight r %}
library(rbenchmark)</p>

<p>benchmark(
  glm = glm(
    admitted ~ exam_score_1 + exam_score_2,
    family = &ldquo;binomial&rdquo;,
    data = ex2data1
    ),
  ucminf = ucminf(
    par = theta,
    fn = function(t) J(X, y, t),
    gr = function(t) gR(X, y, t)
    ),
  ucminf_vectorised = ucminf(
    par = theta,
    fn = function(t) Jv(X, y, t),
    gr = function(t) gRv(X, y, t)
    ),
  replications = 1000,
  columns = c(&ldquo;test&rdquo;,&ldquo;replications&rdquo;,&ldquo;elapsed&rdquo;)
  )
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="test-replications-elapsed">test replications elapsed</h2>

<h2 id="1-glm-1000-4-022">1               glm         1000   4.022</h2>

<h2 id="2-ucminf-1000-0-285">2            ucminf         1000   0.285</h2>

<h2 id="3-ucminf-vectorised-1000-0-254">3 ucminf_vectorised         1000   0.254</h2>

<p>{% endhighlight %}</p>

<p>So even with a relatively small dataset of just 100 rows, we find that a vectorised linear regression solved using an optimisation algorithm is many times quicker than applying a generalised linear model. Kinda makes it all worthwhile!</p>

<p>Next time I&rsquo;ll look at implementing regularisation to fit more complicated decision boundaries.</p>

<p>{% highlight r %}
sessionInfo()
{% endhighlight %}</p>

<p>{% highlight text %}</p>

<h2 id="r-version-3-1-3-2015-03-09">R version 3.1.3 (2015-03-09)</h2>

<h2 id="platform-x86-64-pc-linux-gnu-64-bit">Platform: x86_64-pc-linux-gnu (64-bit)</h2>

<h2 id="running-under-ubuntu-14-04-2-lts">Running under: Ubuntu 14.04.2 LTS</h2>

<h2 id="locale">locale:</h2>

<h2 id="1-lc-ctype-en-gb-utf-8-lc-numeric-c">[1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C</h2>

<h2 id="3-lc-time-en-gb-utf-8-lc-collate-en-gb-utf-8">[3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8</h2>

<h2 id="5-lc-monetary-en-gb-utf-8-lc-messages-en-gb-utf-8">[5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8</h2>

<h2 id="7-lc-paper-en-gb-utf-8-lc-name-c">[7] LC_PAPER=en_GB.UTF-8       LC_NAME=C</h2>

<h2 id="9-lc-address-c-lc-telephone-c">[9] LC_ADDRESS=C               LC_TELEPHONE=C</h2>

<h2 id="11-lc-measurement-en-gb-utf-8-lc-identification-c">[11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C</h2>

<h2 id="attached-base-packages">attached base packages:</h2>

<h2 id="1-methods-stats-graphics-grdevices-utils-datasets-base">[1] methods   stats     graphics  grDevices utils     datasets  base</h2>

<h2 id="other-attached-packages">other attached packages:</h2>

<h2 id="1-rbenchmark-1-0-0-ucminf-1-1-3-ggplot2-1-0-0-magrittr-1-5">[1] rbenchmark_1.0.0 ucminf_1.1-3     ggplot2_1.0.0    magrittr_1.5</h2>

<h2 id="5-dplyr-0-4-1-testthat-0-8-1-knitr-1-9">[5] dplyr_0.4.1      testthat_0.8.1   knitr_1.9</h2>

<h2 id="loaded-via-a-namespace-and-not-attached">loaded via a namespace (and not attached):</h2>

<h2 id="1-assertthat-0-1-colorspace-1-2-5-dbi-0-3-1-digest-0-6-4">[1] assertthat_0.1   colorspace_1.2-5 DBI_0.3.1        digest_0.6.4</h2>

<h2 id="5-evaluate-0-5-5-formatr-1-0-grid-3-1-3-gtable-0-1-2">[5] evaluate_0.5.5   formatR_1.0      grid_3.1.3       gtable_0.1.2</h2>

<h2 id="9-labeling-0-3-lazyeval-0-1-10-mass-7-3-39-munsell-0-4-2">[9] labeling_0.3     lazyeval_0.1.10  MASS_7.3-39      munsell_0.4.2</h2>

<h2 id="13-parallel-3-1-3-plyr-1-8-1-proto-0-3-10-rcpp-0-11-5">[13] parallel_3.1.3   plyr_1.8.1       proto_0.3-10     Rcpp_0.11.5</h2>

<h2 id="17-reshape2-1-4-1-scales-0-2-4-stringr-0-6-2-tcltk-3-1-3">[17] reshape2_1.4.1   scales_0.2.4     stringr_0.6.2    tcltk_3.1.3</h2>

<h2 id="21-tools-3-1-3">[21] tools_3.1.3</h2>

<p>{% endhighlight %}</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

