<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on A Hugo website</title>
    <link>/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on A Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Jan 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Monster Hunting with XGBoost</title>
      <link>/2017/01/04/monster-hunting-with-xgboost/</link>
      <pubDate>Wed, 04 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/04/monster-hunting-with-xgboost/</guid>
      <description>Kaggle&amp;rsquo;s bread-and-butter is its Machine Learning, or Predictive Analytics competitions. The processes used in these scenarios encompass a very small fraction of the Data Science process. However, this gameified version of Data Science can be engaging and may be an interesting hook for some newcomers, as it side-steps some of the more time consuming data wrangling processes. I introduce it here to help those who are looking for a Data Science related resolution or goal.</description>
    </item>
    
    <item>
      <title>Newyearal Networks</title>
      <link>/2017/01/03/newyearal-networks/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/03/newyearal-networks/</guid>
      <description>One of my New Year resolutions is to get to grips with deep learning. I thought a good place to start would be a refresher into &amp;lsquo;shallow&amp;rsquo; neural networks, and that is what this post and the one that follows it will be about. I&amp;rsquo;ll go through the maths behind a quick dirty neural network, and implement it in R.
I&amp;rsquo;m using as my sources the tome Elements of Statistical Learning, Andrew Ng&amp;rsquo;s excellent machine learning course on coursera, and a short course I have been doing on Udemy: Deep Learning in Python.</description>
    </item>
    
    <item>
      <title>Nearest neighbour methods</title>
      <link>/2016/05/27/nearest-neighbour-methods/</link>
      <pubDate>Fri, 27 May 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/05/27/nearest-neighbour-methods/</guid>
      <description>{% include _toc.html %}
In my last post, I started working through some examples given by Hastie et al in Elements of Statistical learning. I looked at using a linear model for classification across a randomly generated training set. In this post I&amp;rsquo;ll use nearest neighbour methods to create a non-linear decision boundary over the same data.
Nearest neighbour algorithm There are much more learned folk than I who give good explanations of the maths behind nearest neighbours, so I won&amp;rsquo;t spend too long on the theory.</description>
    </item>
    
    <item>
      <title>The Game of Kaggle</title>
      <link>/2016/05/21/the-game-of-kaggle/</link>
      <pubDate>Sat, 21 May 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/05/21/the-game-of-kaggle/</guid>
      <description>{% include _toc.html %}
The Player of Games What&amp;rsquo;s the best way to teach oneself machine learning? Is it to do an online course, write a blog or compete in online programming competitions?
This blog post describes my first interaction with / or game of Kaggle.
Kaggle history In 2010, Kaggle was founded as a platform for predictive modelling and analytics competitions on which companies and researchers post their data and statisticians and data miners from all over the world compete to produce the best models.</description>
    </item>
    
    <item>
      <title>Getting to grips with &#39;Elements of statistical learning&#39;</title>
      <link>/2016/05/08/getting-to-grips-with-elements-of-statistical-learning/</link>
      <pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/05/08/getting-to-grips-with-elements-of-statistical-learning/</guid>
      <description>{% include _toc.html %}
Last week I joined a reading group for the weighty tome Elements of Statistical Learning. I really like the idea of this group; interesting as it is - it can be hard to maintain the drive to wade through a text like this. Working through it week on week with a group of like-minded people is a great way to overcome this.
Linear models In this post I implement in R some of the ideas that are presented in the first 2 chapters of Elements of Statistical Learning, namely: least squares.</description>
    </item>
    
    <item>
      <title>Predicting virtual hat sales in Dota 2</title>
      <link>/2016/04/24/predicting-virtual-hat-sales-in-dota-2/</link>
      <pubDate>Sun, 24 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/04/24/predicting-virtual-hat-sales-in-dota-2/</guid>
      <description>{% include _toc.html %}
{% highlight r %} #LIBRARY - check if packages are installed and load them library(dplyr) library(rvest) library(zoo) library(forecast) {% endhighlight %} &amp;gt; &amp;ldquo;Prediction is very difficult, especially if it&amp;rsquo;s about the future.&amp;rdquo; - Niels Bohr
As a scientist I&amp;rsquo;m experienced in designing controlled experiments to produce tidy dataframes. Ideally we end up with each row in the dataframe representing a unique observation at the level of the experimental unit.</description>
    </item>
    
    <item>
      <title>The magic of neural networks</title>
      <link>/2016/04/10/the-magic-of-neural-networks/</link>
      <pubDate>Sun, 10 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/04/10/the-magic-of-neural-networks/</guid>
      <description>{% include _toc.html %}
Education is a key factor affecting long term economic progress. Success in the core subjects provide a linguistic and numeric scaffold for other subjects later in students&amp;rsquo; academic careers.The growth in school educational databases facilitates the use of Data Mining and Machine Learning practises to improve outcomes in these subjects by identifying factors that are indicative of failure (or success). Predicting outcomes allows educators to take corrective measures for weak students mitigating the risk of failure.</description>
    </item>
    
    <item>
      <title>Assessing classifier performance</title>
      <link>/2016/03/24/assessing-classifier-performance/</link>
      <pubDate>Thu, 24 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/03/24/assessing-classifier-performance/</guid>
      <description>{% include _toc.html %}
Introduction  Actual class values
 Predicted class values
 Estimated probability of the prediction
  These are three main types of data that are used to evaluate a classifier. We have used the first two types in previous blogs where we constructed a confusion matrix to compare the actual class values and the predicted class when applying the trained model on the test data with a support vector machines classifier model.</description>
    </item>
    
    <item>
      <title>Logistic regression for student performance prediction</title>
      <link>/2016/03/15/logistic-regression-for-student-performance-prediction/</link>
      <pubDate>Tue, 15 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/03/15/logistic-regression-for-student-performance-prediction/</guid>
      <description>{% include _toc.html %}
Introduction Classification problems occur often, perhaps even more so than regression problems. Consider the Cortez student maths attainment data discussed in previous posts. The response variable, final grade of the year (range 0-20), G3 can be classified into a binary pass or fail variable called final, based on a threshold mark. We used a decision tree approach to model this data before which provided 95% accuracy and had the benefit of interpretability.</description>
    </item>
    
    <item>
      <title>Support vector machines for forest fire prediction</title>
      <link>/2016/03/15/support-vector-machines-for-forest-fire-prediction/</link>
      <pubDate>Tue, 15 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/03/15/support-vector-machines-for-forest-fire-prediction/</guid>
      <description>{% include _toc.html %}
Introduction This post is based on a paper by Cortez &amp;amp; Morais (2007). Forest fires are a major environmental issue, creating economical and ecological damage while endangering human lives. Fast detection is a key element for controlling such phenomenon. To achieve this, one alternative is to use automatic tools based on local sensors, such as microclimate and weather data provided by meteorological stations.
All this data holds valuable information, such as trends and patterns, which can be used to improve decision making.</description>
    </item>
    
    <item>
      <title>Student performance in Portugal</title>
      <link>/2016/03/01/student-performance-in-portugal/</link>
      <pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/03/01/student-performance-in-portugal/</guid>
      <description>{% include _toc.html %}
Education is a key factor affecting long term economic progress. Success in the core languages provide a linguistic and numeric scaffold for other subjects later in students&amp;rsquo; academic careers.The growth in school educational databases facilitates the use of Data Mining and Machine Learning practises to improve outcomes in these subjects by identifying factors that are indicative of failure. Predicting outcomes allows educators to take corrective measures for weak students mitigating the risk of failure.</description>
    </item>
    
    <item>
      <title>Unsupervised learning with K-means</title>
      <link>/2015/11/11/unsupervised-learning-with-k-means/</link>
      <pubDate>Wed, 11 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/11/11/unsupervised-learning-with-k-means/</guid>
      <description>K means is one of the simplest algorithms for unsupervised learning. It works iteratively to cluster together similar values in a dataset of one to many dimensions ($X\in\mathbb{R}^{m \times n}$). The algorithmic steps are simple, it relies only on arithmetic means, so it&amp;rsquo;s pretty simple to understand, but can also be quite powerful. Because it relies on random sampling to initiate the algorithm it can be quite slow however, as there is a need to complete many replications to get a robust result.</description>
    </item>
    
    <item>
      <title>Growing up</title>
      <link>/2015/08/16/growing-up/</link>
      <pubDate>Sun, 16 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/08/16/growing-up/</guid>
      <description>I&amp;rsquo;ve been playing with implementations of linear and logistic regression over the last couple of months, following the exercises from a machine learning course that I have been doing. So far I have been writing things in a very functional way, constantly defining specific functions to do what are essentially generic things.
I&amp;rsquo;ve also started to write a couple of my own packages, one of which I have published on github and zenodo.</description>
    </item>
    
    <item>
      <title>Elastic-net regularisation</title>
      <link>/2015/08/08/elastic-net-regularisation/</link>
      <pubDate>Sat, 08 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/08/08/elastic-net-regularisation/</guid>
      <description>A more formalised implementation of regularisation A short while ago I published a post based on an exercise from Andrew Ng&amp;rsquo;s Machine Learning course on Coursera. In that post I implemented regularisation for a vectorised implementation of linear regression. In reality when approaching a machine learning problem, I wouldn&amp;rsquo;t want to rely on functions I have written myself when there are fully formed packages supporting these techniques. So in this post I&amp;rsquo;m going to reproduce the analysis from my previous post but using the R package glmnet.</description>
    </item>
    
    <item>
      <title>Regularised linear regression</title>
      <link>/2015/07/26/regularised-linear-regression/</link>
      <pubDate>Sun, 26 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/07/26/regularised-linear-regression/</guid>
      <description>In this post I reproduce an example similar to an exercise I did for the coursera MOOC course in machine learning written by Andrew Ng. I&amp;rsquo;m compelting the course musing R, not the requisite matlab. In the next couple of posts I&amp;rsquo;m going to complete the equivalent of exercise 5.
The exercise was about creating a vectorised implementation of regularised linear regression, and using this to test some theory relating to the diagnosis of bias (underfitting) and variance (overfitting).</description>
    </item>
    
    <item>
      <title>Handwriting recognition with logistic regression</title>
      <link>/2015/04/14/handwriting-recognition-with-logistic-regression/</link>
      <pubDate>Tue, 14 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/04/14/handwriting-recognition-with-logistic-regression/</guid>
      <description>In my previous post I completed an exercise using logistic regression to generate complicated non-linear decision boundaries. In this exercise I&amp;rsquo;m going to use much of the same code for handwriting recognition. These exercises are all part of Andrew Ng&amp;rsquo;s Machine Learning course on coursera. All the exercises are done in Matlab/Octave, but I&amp;rsquo;ve been stubborn and have worked solutions in R instead.
The Data For this exercise the dataset comprises 5000 training examples where each examples is a $20 \times 20$ pixel grayscale image of a digit between 0-9.</description>
    </item>
    
    <item>
      <title>Implementing vectorised logistic regression</title>
      <link>/2015/04/06/implementing-vectorised-logistic-regression/</link>
      <pubDate>Mon, 06 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/04/06/implementing-vectorised-logistic-regression/</guid>
      <description>I&amp;rsquo;ve been doing Andrew Ng&amp;rsquo;s excellent Machine Learning course on coursera. The second exercise is to implement from scratch vectorised logistic regression for classification. Submissions to the exercises have to be made in Octave or Matlab; in this post I give the solution using R.
Andrew Ng uses the algorithm fminunc in Matlab/Octave to optimise the logistic regression solution. In R you can use the optim function, but I have been using the ucminf function provided in the package ucminf.</description>
    </item>
    
  </channel>
</rss>