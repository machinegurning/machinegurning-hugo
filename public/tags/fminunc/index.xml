<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fminunc on A Hugo website</title>
    <link>/tags/fminunc/</link>
    <description>Recent content in Fminunc on A Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Apr 2015 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/fminunc/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Handwriting recognition with logistic regression</title>
      <link>/2015/04/14/handwriting-recognition-with-logistic-regression/</link>
      <pubDate>Tue, 14 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/04/14/handwriting-recognition-with-logistic-regression/</guid>
      <description>In my previous post I completed an exercise using logistic regression to generate complicated non-linear decision boundaries. In this exercise I&amp;rsquo;m going to use much of the same code for handwriting recognition. These exercises are all part of Andrew Ng&amp;rsquo;s Machine Learning course on coursera. All the exercises are done in Matlab/Octave, but I&amp;rsquo;ve been stubborn and have worked solutions in R instead.
The Data For this exercise the dataset comprises 5000 training examples where each examples is a $20 \times 20$ pixel grayscale image of a digit between 0-9.</description>
    </item>
    
    <item>
      <title>Non-linear classification with logistic regression</title>
      <link>/2015/04/10/non-linear-classification-with-logistic-regression/</link>
      <pubDate>Fri, 10 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/04/10/non-linear-classification-with-logistic-regression/</guid>
      <description>In my last post I compared vectorised logistic regression solved with an optimisation algorithm with a generalised linear model. I tested it out on a very simple dataset which could be classified using a linear boundary. In this post I&amp;rsquo;m following the next part of Andrew Ng&amp;rsquo;s Machine Learning course on coursera and implementing regularisation and feature mapping to allow me to map non-linear decision boundaries using logistic regression. And of course, I&amp;rsquo;m doing it in R, not Matlab or Octave.</description>
    </item>
    
    <item>
      <title>Implementing vectorised logistic regression</title>
      <link>/2015/04/06/implementing-vectorised-logistic-regression/</link>
      <pubDate>Mon, 06 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/04/06/implementing-vectorised-logistic-regression/</guid>
      <description>I&amp;rsquo;ve been doing Andrew Ng&amp;rsquo;s excellent Machine Learning course on coursera. The second exercise is to implement from scratch vectorised logistic regression for classification. Submissions to the exercises have to be made in Octave or Matlab; in this post I give the solution using R.
Andrew Ng uses the algorithm fminunc in Matlab/Octave to optimise the logistic regression solution. In R you can use the optim function, but I have been using the ucminf function provided in the package ucminf.</description>
    </item>
    
  </channel>
</rss>