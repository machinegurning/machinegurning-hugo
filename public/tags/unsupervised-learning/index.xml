<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Unsupervised Learning on A Hugo website</title>
    <link>/tags/unsupervised-learning/</link>
    <description>Recent content in Unsupervised Learning on A Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Oct 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/unsupervised-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>t-Distributed Stochastic Neighbor Embedding</title>
      <link>/2017/10/13/t-distributed-stochastic-neighbor-embedding/</link>
      <pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/13/t-distributed-stochastic-neighbor-embedding/</guid>
      <description>Last time we looked at the classic approach of PCA, this time we look at a relatively modern method called t-Distributed Stochastic Neighbour Embedding (t-SNE). The paper is fairly accessible so we work through it here and attempt to use the method in R on a new data set (there&amp;rsquo;s also a video talk).
The data science process often starts with visualisation; we want to see the data in an attempt to make sense of it.</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>/2017/09/24/principal-component-analysis/</link>
      <pubDate>Sun, 24 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/24/principal-component-analysis/</guid>
      <description>Sometimes one gets handed so much data you don&amp;rsquo;t know where to begin! You might not even have an associated response variable to complement the hundreds of explanatory variables provided. Instead of attempting to make predictions we can try to make sense of the data using unsupervised learning techniques.
This is challenging in contrast to supervised learning, where there is a simple goal for the analysis, here there is no way to check our work because we don&amp;rsquo;t know the true answer (we have no training set to compare our predictions against).</description>
    </item>
    
    <item>
      <title>Unsupervised learning with K-means</title>
      <link>/2015/11/11/unsupervised-learning-with-k-means/</link>
      <pubDate>Wed, 11 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/11/11/unsupervised-learning-with-k-means/</guid>
      <description>K means is one of the simplest algorithms for unsupervised learning. It works iteratively to cluster together similar values in a dataset of one to many dimensions ($X\in\mathbb{R}^{m \times n}$). The algorithmic steps are simple, it relies only on arithmetic means, so it&amp;rsquo;s pretty simple to understand, but can also be quite powerful. Because it relies on random sampling to initiate the algorithm it can be quite slow however, as there is a need to complete many replications to get a robust result.</description>
    </item>
    
  </channel>
</rss>