<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Logistic Regression on A Hugo website</title>
    <link>/tags/logistic-regression/</link>
    <description>Recent content in Logistic Regression on A Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 21 May 2016 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/logistic-regression/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The Game of Kaggle</title>
      <link>/2016/05/21/the-game-of-kaggle/</link>
      <pubDate>Sat, 21 May 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/05/21/the-game-of-kaggle/</guid>
      <description>{% include _toc.html %}
The Player of Games What&amp;rsquo;s the best way to teach oneself machine learning? Is it to do an online course, write a blog or compete in online programming competitions?
This blog post describes my first interaction with / or game of Kaggle.
Kaggle history In 2010, Kaggle was founded as a platform for predictive modelling and analytics competitions on which companies and researchers post their data and statisticians and data miners from all over the world compete to produce the best models.</description>
    </item>
    
    <item>
      <title>Handwriting recognition with logistic regression</title>
      <link>/2015/04/14/handwriting-recognition-with-logistic-regression/</link>
      <pubDate>Tue, 14 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/04/14/handwriting-recognition-with-logistic-regression/</guid>
      <description>In my previous post I completed an exercise using logistic regression to generate complicated non-linear decision boundaries. In this exercise I&amp;rsquo;m going to use much of the same code for handwriting recognition. These exercises are all part of Andrew Ng&amp;rsquo;s Machine Learning course on coursera. All the exercises are done in Matlab/Octave, but I&amp;rsquo;ve been stubborn and have worked solutions in R instead.
The Data For this exercise the dataset comprises 5000 training examples where each examples is a $20 \times 20$ pixel grayscale image of a digit between 0-9.</description>
    </item>
    
    <item>
      <title>Non-linear classification with logistic regression</title>
      <link>/2015/04/10/non-linear-classification-with-logistic-regression/</link>
      <pubDate>Fri, 10 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/04/10/non-linear-classification-with-logistic-regression/</guid>
      <description>In my last post I compared vectorised logistic regression solved with an optimisation algorithm with a generalised linear model. I tested it out on a very simple dataset which could be classified using a linear boundary. In this post I&amp;rsquo;m following the next part of Andrew Ng&amp;rsquo;s Machine Learning course on coursera and implementing regularisation and feature mapping to allow me to map non-linear decision boundaries using logistic regression. And of course, I&amp;rsquo;m doing it in R, not Matlab or Octave.</description>
    </item>
    
    <item>
      <title>Implementing vectorised logistic regression</title>
      <link>/2015/04/06/implementing-vectorised-logistic-regression/</link>
      <pubDate>Mon, 06 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/04/06/implementing-vectorised-logistic-regression/</guid>
      <description>I&amp;rsquo;ve been doing Andrew Ng&amp;rsquo;s excellent Machine Learning course on coursera. The second exercise is to implement from scratch vectorised logistic regression for classification. Submissions to the exercises have to be made in Octave or Matlab; in this post I give the solution using R.
Andrew Ng uses the algorithm fminunc in Matlab/Octave to optimise the logistic regression solution. In R you can use the optim function, but I have been using the ucminf function provided in the package ucminf.</description>
    </item>
    
  </channel>
</rss>